[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/yufeiwu/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(here)\n\n# Load the dataset\ndata &lt;- read.csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\n# Create the plot with facets by DOSE, showing both lines and points\nggplot(data, aes(x = TIME, y = DV, group = ID)) +\n  geom_line(color = \"lightblue\") +  # Add lines\n  geom_point(color = \"skyblue\") +  # Add points\n  facet_wrap(~ DOSE) +  # Facet by dose with the same y-axis scale\n  labs(title = \"DV over Time Stratified by DOSE\",\n       x = \"Time\",\n       y = \"DV\") +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n# Filter to keep only observations where OCC == 1\ndata1 &lt;- data %&gt;% filter(OCC == 1)\n\n# Exclude observations where TIME = 0 and compute the sum of DV for each individual\ndata_sum &lt;- data1 %&gt;%\n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV))  # Sum DV for each ID\n\ndim(data_sum)\n\n[1] 120   2\n\n# Create a data frame with only observations where TIME == 0\ndata_time0 &lt;- data1 %&gt;%\n  filter(TIME == 0)\n\ndim(data_time0)\n\n[1] 120  17\n\n# Join the two data frames by ID to create the final dataset\njoint_data &lt;- left_join(data_time0, data_sum, by = \"ID\")\n\ndim(joint_data)\n\n[1] 120  18\n\n# Convert RACE and SEX to factors and keep only the required columns\nfinal_data &lt;- joint_data %&gt;%\n  mutate(RACE = as.factor(RACE),\n         SEX = as.factor(SEX)) %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\n# View the resulting data frame\nhead(final_data)\n\n        Y DOSE AGE SEX RACE   WT       HT\n1 2690.52   25  42   1    2 94.3 1.769997\n2 2638.81   25  24   1    2 80.4 1.759850\n3 2149.61   25  31   1    1 71.8 1.809847\n4 1788.89   25  46   2    1 77.4 1.649993\n5 3126.37   25  41   2    2 64.3 1.560052\n6 2336.89   25  27   1    2 74.1 1.829862"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#data-processing-and-exploration",
    "href": "fitting-exercise/fitting-exercise.html#data-processing-and-exploration",
    "title": "Fitting Exercise",
    "section": "",
    "text": "# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(here)\n\n# Load the dataset\ndata &lt;- read.csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\n# Create the plot with facets by DOSE, showing both lines and points\nggplot(data, aes(x = TIME, y = DV, group = ID)) +\n  geom_line(color = \"lightblue\") +  # Add lines\n  geom_point(color = \"skyblue\") +  # Add points\n  facet_wrap(~ DOSE) +  # Facet by dose with the same y-axis scale\n  labs(title = \"DV over Time Stratified by DOSE\",\n       x = \"Time\",\n       y = \"DV\") +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n# Filter to keep only observations where OCC == 1\ndata1 &lt;- data %&gt;% filter(OCC == 1)\n\n# Exclude observations where TIME = 0 and compute the sum of DV for each individual\ndata_sum &lt;- data1 %&gt;%\n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV))  # Sum DV for each ID\n\ndim(data_sum)\n\n[1] 120   2\n\n# Create a data frame with only observations where TIME == 0\ndata_time0 &lt;- data1 %&gt;%\n  filter(TIME == 0)\n\ndim(data_time0)\n\n[1] 120  17\n\n# Join the two data frames by ID to create the final dataset\njoint_data &lt;- left_join(data_time0, data_sum, by = \"ID\")\n\ndim(joint_data)\n\n[1] 120  18\n\n# Convert RACE and SEX to factors and keep only the required columns\nfinal_data &lt;- joint_data %&gt;%\n  mutate(RACE = as.factor(RACE),\n         SEX = as.factor(SEX)) %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\n# View the resulting data frame\nhead(final_data)\n\n        Y DOSE AGE SEX RACE   WT       HT\n1 2690.52   25  42   1    2 94.3 1.769997\n2 2638.81   25  24   1    2 80.4 1.759850\n3 2149.61   25  31   1    1 71.8 1.809847\n4 1788.89   25  46   2    1 77.4 1.649993\n5 3126.37   25  41   2    2 64.3 1.560052\n6 2336.89   25  27   1    2 74.1 1.829862"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#eda",
    "href": "fitting-exercise/fitting-exercise.html#eda",
    "title": "Fitting Exercise",
    "section": "EDA",
    "text": "EDA\nFirst make summary table of the variables:\n\n# Load package\nlibrary(gtsummary)\n\n# Summary statistics for variables\ntbl_summary(final_data)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1201\n\n\n\n\nY\n2,349 (1,689, 3,054)\n\n\nDOSE\n\n\n\n\n    25\n59 (49%)\n\n\n    37.5\n12 (10%)\n\n\n    50\n49 (41%)\n\n\nAGE\n31 (26, 41)\n\n\nSEX\n\n\n\n\n    1\n104 (87%)\n\n\n    2\n16 (13%)\n\n\nRACE\n\n\n\n\n    1\n74 (62%)\n\n\n    2\n36 (30%)\n\n\n    7\n2 (1.7%)\n\n\n    88\n8 (6.7%)\n\n\nWT\n82 (73, 90)\n\n\nHT\n1.77 (1.70, 1.82)\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n\n\n\n\n\n\nThe table provides an overview of the data, highlighting three dose treatments: 25, 37.5, and 50. The high and low doses make up the majority, while the medium dose (37.5) accounts for only 12% of the observations. Additionally, there are four different race categories, encoded as 1, 2, 7, and 88, with categories 1 and 2 being the most prevalent.\n\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a function to generate histograms\nplot_histogram &lt;- function(data, var, binwidth = NULL) {\n  ggplot(data, aes(x = !!sym(var))) +\n    geom_histogram(aes(y = ..density..), fill = \"skyblue\", color = \"black\", bins = 30) +\n    geom_density(color = \"pink\", linewidth = 1) +  # Overlay density curve\n    labs(title = paste(\"Histogram of\", var), x = var, y = \"Density\") +\n    theme_minimal()\n}\n\n# Plot histograms for Y, AGE, WT, and HT\nhist_Y &lt;- plot_histogram(final_data, \"Y\")\nhist_AGE &lt;- plot_histogram(final_data, \"AGE\")\nhist_WT &lt;- plot_histogram(final_data, \"WT\")\nhist_HT &lt;- plot_histogram(final_data, \"HT\")\n\n# Print plots\nprint(hist_Y)\n\n\n\n\n\n\n\nprint(hist_AGE)\n\n\n\n\n\n\n\nprint(hist_WT)\n\n\n\n\n\n\n\nprint(hist_HT)\n\n\n\n\n\n\n\n\nY exhibits a right-skewed distribution, indicating that most values are concentrated on the lower end, with a few higher values extending the tail to the right. AGE follows a bimodal distribution, suggesting the presence of two distinct age groups in the dataset. WT (Weight) appears to be approximately normally distributed. HT (Height) shows a left-skewed distribution, where most values are on the higher end, with a tail extending towards lower values.\nNow make some scatterplots/boxplots between Y and other predictors.\n\n# Load necessary library\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Boxplots for Y vs categorical variables\nboxplot_dose &lt;- ggplot(final_data, aes(x = as.factor(DOSE), y = Y)) + #although DOSE is a continuous variable, it only has three values, so box plot will be clearer to display the relationship\n  geom_boxplot(fill = \"pink\", color = \"black\") +\n  labs(title = \"Boxplot of Y vs Dose\", x = \"Dose\", y = \"Y\") +\n  theme_minimal()\n\nboxplot_sex &lt;- ggplot(final_data, aes(x = SEX, y = Y)) +\n  geom_boxplot(fill = \"pink\", color = \"black\") +\n  labs(title = \"Boxplot of Y vs Sex\", x = \"Sex\", y = \"Y\") +\n  theme_minimal()\n\nboxplot_race &lt;- ggplot(final_data, aes(x = RACE, y = Y)) +\n  geom_boxplot(fill = \"pink\", color = \"black\") +\n  labs(title = \"Boxplot of Y vs Race\", x = \"Race\", y = \"Y\") +\n  theme_minimal()\n\n# Scatter plots for Y vs continuous variables\nscatter_age &lt;- ggplot(final_data, aes(x = AGE, y = Y)) +\n  geom_point(alpha = 0.6, color = \"orange\") +\n  geom_smooth(method = \"lm\", color = \"black\", se = FALSE) +  # Add trend line\n  labs(title = \"Scatter Plot of Y vs Age\", x = \"Age\", y = \"Y\") +\n  theme_minimal()\n\nscatter_wt &lt;- ggplot(final_data, aes(x = WT, y = Y)) +\n  geom_point(alpha = 0.6, color = \"orange\") +\n  geom_smooth(method = \"lm\", color = \"black\", se = FALSE) +\n  labs(title = \"Scatter Plot of Y vs Weight\", x = \"Weight\", y = \"Y\") +\n  theme_minimal()\n\nscatter_ht &lt;- ggplot(final_data, aes(x = HT, y = Y)) +\n  geom_point(alpha = 0.6, color = \"orange\") +\n  geom_smooth(method = \"lm\", color = \"black\", se = FALSE) +\n  labs(title = \"Scatter Plot of Y vs Height\", x = \"Height\", y = \"Y\") +\n  theme_minimal()\n\n# Print the plots\nprint(boxplot_dose)\n\n\n\n\n\n\n\nprint(boxplot_sex)\n\n\n\n\n\n\n\nprint(boxplot_race)\n\n\n\n\n\n\n\nprint(scatter_age)\n\n\n\n\n\n\n\nprint(scatter_wt)\n\n\n\n\n\n\n\nprint(scatter_ht)\n\n\n\n\n\n\n\n\nFrom the plots, we observe a positive correlation between Y and dose. However, sex and race do not appear to have a significant impact on Y. There is no clear correlation between age and Y, while weight and height show weak negative correlations with Y.\nMake pair plots and correlation matrix.\n\n# Load packages\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(corrplot)\n\n# Generate a pairwise plot (GGally's ggpairs)\nggpairs(final_data, \n        mapping = aes(color = SEX),  # Color by SEX\n        upper = list(continuous = \"cor\"),  # Show correlation in upper panel\n        lower = list(continuous = \"smooth\"),  # Smoothed scatter plots in lower panel\n        diag = list(continuous = \"density\"))  # Density plots on the diagonal\n\n\n\n\n\n\n\n# Compute correlation matrix\nnumeric_vars &lt;- final_data %&gt;% select(where(is.numeric))  # Select only numeric columns\ncor_matrix &lt;- cor(numeric_vars, use = \"complete.obs\")  # Compute correlations\n\n# Plot correlation matrix\ncorrplot(cor_matrix, method = \"color\", type = \"lower\", addCoef.col = \"black\",\n         tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nFrom the correlation matrix, there appear to be potential correlations between Y and DOSE, as well as between height (HT) and weight (WT)."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-fitting",
    "href": "fitting-exercise/fitting-exercise.html#model-fitting",
    "title": "Fitting Exercise",
    "section": "Model fitting",
    "text": "Model fitting\n\nLinear Model\n\nFit a linear model to Y using the main predictor DOSE.\nFit a linear model to Y using all predictors.\nCompute RMSE and R-squared and print them.\n\n\n# Load packages\nlibrary(tidymodels)\nlibrary(dplyr)\n\n# Set up a workflow for model1: Y ~ DOSE\nmodel1_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nmodel1_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ DOSE) %&gt;%\n  add_model(model1_spec)\n\n# Fit the model1\nmodel1_fit &lt;- fit(model1_workflow, data = final_data)\n\n# Set up a workflow for model2: Y ~ all predictors\nmodel2_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nmodel2_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ .) %&gt;%\n  add_model(model2_spec)\n\n# Fit the model2\nmodel2_fit &lt;- fit(model2_workflow, data = final_data)\n\n# Tidy the model summaries (coefficients)\ntidy_model1 &lt;- tidy(model1_fit)\ntidy_model2 &lt;- tidy(model2_fit)\n\n# Print nicely\nprint(tidy_model1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    323.     199.        1.62 1.07e- 1\n2 DOSE            58.2      5.19     11.2  2.69e-20\n\nprint(tidy_model2)\n\n# A tibble: 9 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  3387.     1835.       1.85  6.76e- 2\n2 DOSE           59.9       4.88    12.3   2.05e-22\n3 AGE             3.16      7.82     0.403 6.88e- 1\n4 SEX2         -358.      217.      -1.65  1.02e- 1\n5 RACE2         155.      129.       1.21  2.31e- 1\n6 RACE7        -405.      448.      -0.904 3.68e- 1\n7 RACE88        -53.5     245.      -0.219 8.27e- 1\n8 WT            -23.0       6.40    -3.60  4.71e- 4\n9 HT           -748.     1104.      -0.678 4.99e- 1\n\n# Compute performance metrics for model1 and model2\nmetrics1 &lt;- model1_fit %&gt;% \n  predict(final_data) %&gt;% \n  bind_cols(final_data) %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\nmetrics2 &lt;- model2_fit %&gt;% \n  predict(final_data) %&gt;% \n  bind_cols(final_data) %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\n# Extract and print RMSE and R-squared for both models\ncat(\"Model 1 (Y ~ DOSE):\\n\")\n\nModel 1 (Y ~ DOSE):\n\ncat(\"RMSE:\", round(metrics1 %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate), 2), \"\\n\")\n\nRMSE: 666.46 \n\ncat(\"R-squared:\", round(metrics1 %&gt;% filter(.metric == \"rsq\") %&gt;% pull(.estimate), 4), \"\\n\\n\")\n\nR-squared: 0.5156 \n\ncat(\"Model 2 (Y ~ all predictors):\\n\")\n\nModel 2 (Y ~ all predictors):\n\ncat(\"  RMSE:\", round(metrics2 %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate), 2), \"\\n\")\n\n  RMSE: 590.85 \n\ncat(\"  R-squared:\", round(metrics2 %&gt;% filter(.metric == \"rsq\") %&gt;% pull(.estimate), 4), \"\\n\")\n\n  R-squared: 0.6193 \n\n\nFrom the linear model Y ~ DOSE, we observe a strong positive correlation between Y and dose, with a coefficient of 58.2. The model’s RMSE is 666.46, and the R-squared value is 0.5156.\nIn the full model (Y ~ all predictors), the coefficient of DOSE remains similar to the first model (59.9 vs. 58.2), suggesting its strong influence on Y. Additionally, weight (WT) shows a negative correlation with Y, with a coefficient of -23.0. This model performs better, with a lower RMSE of 590.85 and a higher R-squared of 0.6193, indicating that incorporating additional predictors improves the model’s explanatory power.\n\n\nLogistic Model\nFit a logistic model to the SEX using the main predictor of interest DOSE. Fit a logistic model to SEX using all predictors. For both models, compute accuracy and ROC-AUC and print them.\n\n# Load packages\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(yardstick)\n\n# Fit the first logistic regression model: SEX ~ DOSE\nlogit_model1 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(SEX ~ DOSE, final_data)\n\n# Fit the second logistic regression model: SEX ~ all predictors\nlogit_model2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(SEX ~ DOSE + AGE + RACE + WT + HT, final_data)\n\n# Tidy summary of both models\ntidy_logit_model1 &lt;- tidy(logit_model1)\ntidy_logit_model2 &lt;- tidy(logit_model2)\n\n# Print the tidy summaries\nprint(tidy_logit_model1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.765     0.854     -0.896   0.370\n2 DOSE         -0.0318    0.0243    -1.31    0.192\n\nprint(tidy_logit_model2)\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  59.7      18.1        3.30  0.000984\n2 DOSE         -0.101     0.0537    -1.88  0.0604  \n3 AGE           0.0797    0.0583     1.37  0.172   \n4 RACE2        -2.09      1.30      -1.61  0.108   \n5 RACE7         0.614     3.18       0.193 0.847   \n6 RACE88       -1.17      2.08      -0.562 0.574   \n7 WT           -0.0141    0.0657    -0.214 0.830   \n8 HT          -35.0      11.5       -3.06  0.00225 \n\n# Compute performance metrics\nmetrics3 &lt;- predict(logit_model1, final_data, type = \"class\") %&gt;%\n  bind_cols(predict(logit_model1, final_data, type = \"prob\")) %&gt;%\n  bind_cols(final_data) %&gt;%\n  metrics(truth = SEX, estimate = .pred_class, .pred_1)\n\n# Compute performance metrics\nmetrics4 &lt;- predict(logit_model2, final_data, type = \"class\") %&gt;%\n  bind_cols(predict(logit_model2, final_data, type = \"prob\")) %&gt;%\n  bind_cols(final_data) %&gt;%\n  metrics(truth = SEX, estimate = .pred_class, .pred_1)\n\n# Extract and print accuracy and ROC-AUC for both models\ncat(\"Model 1 (Y ~ DOSE):\\n\")\n\nModel 1 (Y ~ DOSE):\n\ncat(\"Accuracy:\", round(metrics3 %&gt;% filter(.metric == \"accuracy\") %&gt;% pull(.estimate), 2), \"\\n\")\n\nAccuracy: 0.87 \n\ncat(\"ROC-AUC:\", round(metrics3 %&gt;% filter(.metric == \"roc_auc\") %&gt;% pull(.estimate), 4), \"\\n\\n\")\n\nROC-AUC: 0.5919 \n\ncat(\"Model 2 (Y ~ all predictors):\\n\")\n\nModel 2 (Y ~ all predictors):\n\ncat(\"Accuracy:\", round(metrics4 %&gt;% filter(.metric == \"accuracy\") %&gt;% pull(.estimate), 2), \"\\n\")\n\nAccuracy: 0.94 \n\ncat(\"ROC-AUC:\", round(metrics4 %&gt;% filter(.metric == \"roc_auc\") %&gt;% pull(.estimate), 4), \"\\n\")\n\nROC-AUC: 0.9754 \n\n\nFrom the logistic model SEX ~ DOSE, the variable DOSE does not significantly influence the prediction of SEX. The model’s accuracy is 0.87, meaning that 87% of the predictions were correct. However, the ROC-AUC value of 0.5919 indicates that the model has poor discriminatory ability, as it is only slightly better than random guessing.\nFrom the logistic model SEX ~ all predictors, which includes variables such as DOSE, AGE, RACE, WT, and HT, the variable HEIGHT shows a negative correlation with SEX, with a coefficient of -35.0. The accuracy of this model is 0.94, suggesting a substantial improvement in prediction accuracy compared to the first model. This improvement implies that the additional predictors provide more relevant information for predicting SEX, leading to more correct predictions. The ROC-AUC value of 0.9754, which is much closer to 1, indicates that the model performs excellently in distinguishing between the two classes."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-1",
    "href": "fitting-exercise/fitting-exercise.html#part-1",
    "title": "Fitting Exercise",
    "section": "Part 1",
    "text": "Part 1\nModel performance assessment 1\n\n# Load packages\nlibrary(tidymodels)\nlibrary(dplyr)\n\n# Define a seed\nrngseed = 1234\n\n# Remove the race vairable\ndata_fit &lt;- final_data %&gt;%\n  select(Y, DOSE, AGE, SEX, WT, HT)\n\n# Set a seed\nset.seed(rngseed)\n\n# Split the data\ndata_split &lt;- initial_split(data_fit, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n# Set up a workflow for model5: Y ~ DOSE\nmodel5_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nmodel5_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ DOSE) %&gt;%\n  add_model(model5_spec)\n\n# Fit model5 on the training data\nmodel5_fit &lt;- fit(model5_workflow, data = train_data)\n\n# Set up a workflow for model6: Y ~ all predictors\nmodel6_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nmodel6_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ .) %&gt;%\n  add_model(model6_spec)\n\n# Fit model6 on the training data\nmodel6_fit &lt;- fit(model6_workflow, data = train_data)\n\n# Compute predictions and RMSE for Model 5\nmetrics5 &lt;- model5_fit %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\n# Compute predictions and RMSE for Model 6\nmetrics6 &lt;- model6_fit %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\n# Compute RMSE for the null model (predicting mean Y)\nnull_rmse &lt;- train_data %&gt;%\n  mutate(pred_null = mean(Y)) %&gt;%\n  metrics(truth = Y, estimate = pred_null) %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  pull(.estimate)\n\n# Extract RMSE values for comparison\nrmse5 &lt;- metrics5 %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate)\nrmse6 &lt;- metrics6 %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate)\n\n# Print results\ncat(\"Model 5 (Y ~ DOSE):\\n\")\n\nModel 5 (Y ~ DOSE):\n\ncat(\"  RMSE:\", round(rmse5, 2), \"\\n\")\n\n  RMSE: 702.81 \n\ncat(\"Model 6 (Y ~ all predictors):\\n\")\n\nModel 6 (Y ~ all predictors):\n\ncat(\"  RMSE:\", round(rmse6, 2), \"\\n\")\n\n  RMSE: 627.44 \n\ncat(\"Null Model (predicting mean Y):\\n\")\n\nNull Model (predicting mean Y):\n\ncat(\"  RMSE:\", round(null_rmse, 2), \"\\n\")\n\n  RMSE: 948.35 \n\n\nComparing the RMSE of three models, we can tell that the second model (Y ~ all predictors) performs the best with the lowest RMSE (627.44). In contrast, the RMSE of the first model (Y~ DOSE) is 702.81 and that of null model is 948.35.\nModel performance assessment 2\n\n# Set a seed\nset.seed(rngseed)\n\n# Create 10-fold cross-validation on the TRAINING DATA ONLY\ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\n# Define model specifications\nmodel5_cv_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nmodel6_cv_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# Create workflows\nmodel5_cv_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ DOSE) %&gt;%\n  add_model(model5_cv_spec)\n\nmodel6_cv_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ .) %&gt;%\n  add_model(model6_cv_spec)\n\n# Perform 10-fold cross-validation for model5_cv on training data\ncv_results5_cv &lt;- fit_resamples(\n  model5_cv_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(rmse),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Perform 10-fold cross-validation for model6_cv on training data\ncv_results6_cv &lt;- fit_resamples(\n  model6_cv_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(rmse),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Extract RMSE estimates\nrmse5_cv_summary &lt;- collect_metrics(cv_results5_cv) %&gt;% filter(.metric == \"rmse\")\nrmse6_cv_summary &lt;- collect_metrics(cv_results6_cv) %&gt;% filter(.metric == \"rmse\")\n\n# Compute mean RMSE and standard error\nrmse5_cv_mean &lt;- rmse5_cv_summary$mean\nrmse5_cv_se &lt;- rmse5_cv_summary$std_err\n\nrmse6_cv_mean &lt;- rmse6_cv_summary$mean\nrmse6_cv_se &lt;- rmse6_cv_summary$std_err\n\n# Compute RMSE for the null model (same as before, using train_data)\nnull_rmse &lt;- train_data %&gt;%\n  mutate(pred_null = mean(Y)) %&gt;%\n  metrics(truth = Y, estimate = pred_null) %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  pull(.estimate)\n\n# Print results\ncat(\"10-Fold Cross-Validation RMSE:\\n\")\n\n10-Fold Cross-Validation RMSE:\n\ncat(\"Model 5 (Y ~ DOSE):\\n\")\n\nModel 5 (Y ~ DOSE):\n\ncat(\"  Mean RMSE:\", round(rmse5_cv_mean, 2), \"\\n\")\n\n  Mean RMSE: 690.54 \n\ncat(\"  Standard Error:\", round(rmse5_cv_se, 4), \"\\n\\n\")\n\n  Standard Error: 67.4951 \n\ncat(\"Model 6 (Y ~ all predictors):\\n\")\n\nModel 6 (Y ~ all predictors):\n\ncat(\"  Mean RMSE:\", round(rmse6_cv_mean, 2), \"\\n\")\n\n  Mean RMSE: 645.69 \n\ncat(\"  Standard Error:\", round(rmse6_cv_se, 4), \"\\n\\n\")\n\n  Standard Error: 64.8193 \n\ncat(\"Null Model (predicting mean Y, on training data):\\n\")\n\nNull Model (predicting mean Y, on training data):\n\ncat(\"  RMSE:\", round(null_rmse, 2), \"\\n\\n\")\n\n  RMSE: 948.35 \n\n\nWith cross-validation, the RMSE of the first model (Y ~ DOSE) is 690.54 (standard error 67.4951), lower than without cross-validation (RMSE 702.81). The RMSE of the second model (Y ~ all predictors) is 645.69 (standard error 64.8193), higher than without cross-validation (RMSE 627.44). The RMSE of the null model remains the same (948.35). This time, the second model (Y ~ all predictors) still performs the best with the lowest RMSE. The standard errors of RMSE for both models are around 10% of mean, which suggests some variability in model performance across different folds.\nChoose a different value for the random seed\n\n# Set a seed\nset.seed(111)\n\n# Create 10-fold cross-validation on the TRAINING DATA ONLY\ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\n# Define model specifications\nmodel5_cv_new_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nmodel6_cv_new_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# Create workflows\nmodel5_cv_new_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ DOSE) %&gt;%\n  add_model(model5_cv_new_spec)\n\nmodel6_cv_new_workflow &lt;- workflow() %&gt;% \n  add_formula(Y ~ .) %&gt;%\n  add_model(model6_cv_new_spec)\n\n# Perform 10-fold cross-validation for model5_cv_new on training data\ncv_results5_cv_new &lt;- fit_resamples(\n  model5_cv_new_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(rmse),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Perform 10-fold cross-validation for model6_cv_new on training data\ncv_results6_cv_new &lt;- fit_resamples(\n  model6_cv_new_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(rmse),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Extract RMSE estimates\nrmse5_cv_new_summary &lt;- collect_metrics(cv_results5_cv_new) %&gt;% filter(.metric == \"rmse\")\nrmse6_cv_new_summary &lt;- collect_metrics(cv_results6_cv_new) %&gt;% filter(.metric == \"rmse\")\n\n# Compute mean RMSE and standard error\nrmse5_cv_new_mean &lt;- rmse5_cv_new_summary$mean\nrmse5_cv_new_se &lt;- rmse5_cv_new_summary$std_err\n\nrmse6_cv_new_mean &lt;- rmse6_cv_new_summary$mean\nrmse6_cv_new_se &lt;- rmse6_cv_new_summary$std_err\n\n# Compute RMSE for the null model (same as before, using train_data)\nnull_rmse &lt;- train_data %&gt;%\n  mutate(pred_null = mean(Y)) %&gt;%\n  metrics(truth = Y, estimate = pred_null) %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  pull(.estimate)\n\n# Print results\ncat(\"10-Fold Cross-Validation RMSE:\\n\")\n\n10-Fold Cross-Validation RMSE:\n\ncat(\"Model 5 (Y ~ DOSE):\\n\")\n\nModel 5 (Y ~ DOSE):\n\ncat(\"  Mean RMSE:\", round(rmse5_cv_new_mean, 2), \"\\n\")\n\n  Mean RMSE: 682.35 \n\ncat(\"  Standard Error:\", round(rmse5_cv_new_se, 4), \"\\n\\n\")\n\n  Standard Error: 65.5072 \n\ncat(\"Model 6 (Y ~ all predictors):\\n\")\n\nModel 6 (Y ~ all predictors):\n\ncat(\"  Mean RMSE:\", round(rmse6_cv_new_mean, 2), \"\\n\")\n\n  Mean RMSE: 652.19 \n\ncat(\"  Standard Error:\", round(rmse6_cv_new_se, 4), \"\\n\\n\")\n\n  Standard Error: 46.5851 \n\ncat(\"Null Model (predicting mean Y, on training data):\\n\")\n\nNull Model (predicting mean Y, on training data):\n\ncat(\"  RMSE:\", round(null_rmse, 2), \"\\n\\n\")\n\n  RMSE: 948.35 \n\n\nAfter the seed was changed from 1234 to 111, the RMSE values changed. The RMSE of the first model (Y ~ DOSE) is 682.35 (standard error 65.5072), and the RMSE of the second model (Y ~ all predictors) is 652.19 (standard error 46.5851), with the null model remaining the same. Still, the second model (Y ~ all predictors) performs the best with the lowest RMSE."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-two-this-section-was-added-by-natalie-cann",
    "href": "fitting-exercise/fitting-exercise.html#part-two-this-section-was-added-by-natalie-cann",
    "title": "Fitting Exercise",
    "section": "Part Two: This section was added by Natalie Cann",
    "text": "Part Two: This section was added by Natalie Cann\nPackages:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nModel Predictions\nFirst, I will put the observed and predicted values from teh 3 original model fits to all of the training data into a data frame.\n\n# dataframe with observed and predicted values\npredictions_df &lt;- bind_rows(\n  train_data %&gt;% \n    mutate(pred = predict(model5_fit, train_data)$.pred, model = \"Dose Model\"), # Model 5 is just dose\n  train_data %&gt;% \n    mutate(pred = predict(model6_fit, train_data)$.pred, model = \"All Predictors Model\"), # Model 6 is all predictors\n  train_data %&gt;% \n    mutate(pred = mean(train_data$Y), model = \"Null Model\")\n)\n\nNow, I will use ggplot to create a figure plotting the observed values on the x-axis and the predictions on the y-axis. Each color in the graph will represent a different model.\n\nggplot(predictions_df, aes(x = Y, y = pred, color = model, shape = model)) +\n  geom_point(alpha = 0.6) + \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + # 45 degree line\n  labs(\n    x = \"Observed Values\",\n    y = \"Predicted Values\",\n    title = \"Observed vs Predicted Values\"\n  ) +\n  scale_x_continuous(limits = c(0, 5000)) +\n  scale_y_continuous(limits = c(0, 5000)) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5), # bold and larger title\n    axis.title = element_text(face = \"bold\", size = 14) # bold and larger axis labels\n  )\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNow, I will create the graph above with facets.\n\nggplot(predictions_df, aes(x = Y, y = pred, color = model, shape = model)) +\n  geom_point(alpha = 0.6) + \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + # 45 degree line\n  labs(\n    x = \"Observed Values\",\n    y = \"Predicted Values\",\n    title = \"Observed vs Predicted Values by Model\"\n  ) +\n  scale_x_continuous(limits = c(0, 5000)) +\n  scale_y_continuous(limits = c(0, 5000)) +\n  facet_wrap(~ model) + # adding facets!\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5), # bold and larger title\n    axis.title = element_text(face = \"bold\", size = 14), # bold and larger axis labels\n    axis.text = element_text(size = 5) # smaller axis text so it can be seen\n  )\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nFor the facet grid of the all predictors model, the points appear to be scattered around the 45 degree horizontal line. For the dose model, there appear to be three horizontal lines upon which the points fall. This makes sense because there are only 3 values for dose (show below to be: 25.0, 37.5, 50.0). For the null model, there is a single horizontal line that the points fall upon. This makes sense because we are predicting the exact same value for each observation.\n\nunique(data$DOSE)\n\n[1] 25.0 37.5 50.0\n\n\nThe model with all the predictors looks the best. To see if there are patterns, I will now plot the predicted vs. the residuals for this model (all predictors model).\n\n# all predictors model \n# compute predicted and residuals \nall_preds &lt;- model6_fit %&gt;% \n  predict(train_data) %&gt;% \n  bind_cols(train_data) %&gt;%\n  mutate(residuals = .pred - Y)\n\n# plot predicted vs residuals\nggplot(all_preds, aes(x = .pred, y = residuals)) +\n  geom_point(alpha = 0.6, color = \"lightskyblue\") + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"lightpink2\") + # line at y = 0\n  labs(\n    x = \"Predicted Values\",\n    y = \"Residuals\",\n    title = \"Predicted vs Residuals for All Predictors Model\"\n  ) +\n  scale_y_continuous(limits = c(-1500, 1500)) + # scale for same pos/neg direction\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5), \n    axis.title = element_text(face = \"bold\", size = 14), \n    axis.text = element_text(size = 10) \n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere appears to be some sort of pattern in the plot above. In particular, there appears to be more and higher negative values compared to positive ones. This suggests either: 1) we’re missing important information (variables), 2) the model is too simple.\n\n\nModel Predictions and Uncertainty\nFocusing on model 2:\n\n# setting seed \nset.seed(rngseed) \n\n# create 100 bootstraps (with bootstraps function from rsample package)\nlibrary(rsample)\ndat_bs &lt;- bootstraps(train_data, times = 100)\n\nlibrary(purrr)\n\n# initialize a list to store predictions\npred_bs &lt;- vector(\"list\", length = 100)\n\n# loop through each bootstrap sample\nfor (i in seq_along(dat_bs$splits)) {\n  dat_sample &lt;- rsample::analysis(dat_bs$splits[[i]])\n  model_fit &lt;- lm(Y ~ ., data = dat_sample)\n  pred_bs[[i]] &lt;- predict(model_fit, newdata = train_data)\n}\n\n# convert list to matrix\npred_bs &lt;- do.call(cbind, pred_bs)\n\npreds &lt;- apply(pred_bs, 1, quantile, c(0.055, 0.5, 0.945))\npreds &lt;- t(preds)\ncolnames(preds) &lt;- c(\"lower\", \"median\", \"upper\")\n\nLastly, I will make a graph that plots observed values on the x-axis and point estimate, median, and upper & lower bounds on the y-axis.\n\n# combine observed values and predictions\nplot_data &lt;- data.frame(\n  observed = train_data$Y,\n  point_estimate = predict(lm(Y ~ ., data = train_data), newdata = train_data),\n  median = preds[, \"median\"],\n  lower = preds[, \"lower\"],\n  upper = preds[, \"upper\"]\n)\n\nggplot(plot_data, aes(x = observed)) +\n  geom_point(aes(y = point_estimate, color = \"Point Estimate\"), size = 2) +\n  geom_point(aes(y = median, color = \"Median\"), size = 2, shape = 15) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = \"89% Confidence Interval\"), width = 0.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  scale_color_manual(\n    values = c(\"Point Estimate\" = \"deepskyblue\", \"Median\" = \"hotpink2\", \"89% Confidence Interval\" = \"lightgreen\"),\n    name = \"Legend\"\n  ) +\n  labs(\n    x = \"Observed Values\",\n    y = \"Predicted Values\",\n    title = \"Observed vs Predicted Values with Uncertainty\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5), # bold and larger title\n    axis.title = element_text(face = \"bold\", size = 14), # bold and larger axis labels\n    axis.text = element_text(size = 10) # larger axis text for better visibility\n  )\n\n\n\n\n\n\n\n\nThe graph above displays several points that lie closer to the diagonal line and several points that lie further from the diagonal line. The points that lie closer to the line indicate that this prediction is closer to the observed (actual) value. The points that lie further from the line indicate that the prediction is further from the observed value.\nMore narrow error bars are better than wider error bars. When the error bar crosses over the diagonal line, it is possible that the true value could be either overestimated or underestimated by the model.\nWhen the point estimates and medians are closer, it is indicated that the bootstrap sampling method did not significantly alter the central tendency of the predictions. When we see point estimates and medians that are far away from each other, we should consider that this may have occurred due to bias.\nIn the graph above, we see that most point estimates and medians are similar to each other. There is a large number of points for which their 89% confidence interval does not cross teh diagonal line, which is a good sign (suggests that most of the 89% CIs are statistically significant). However, it can be noted that the cluster of points at the top right corner of the graph suggest that the model’s predicted value of Y appears to be underestimated when compared to the observed value of Y."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-3",
    "href": "fitting-exercise/fitting-exercise.html#part-3",
    "title": "Fitting Exercise",
    "section": "Part 3",
    "text": "Part 3\n\nFinal evaluation using test data\n\n# Predict on training data\ntrain_predictions &lt;- predict(model6_fit, train_data) %&gt;%\n  bind_cols(train_data %&gt;% select(Y)) %&gt;%\n  mutate(Set = \"Training\")\n\n# Predict on test data\ntest_predictions &lt;- predict(model6_fit, test_data) %&gt;%\n  bind_cols(test_data %&gt;% select(Y)) %&gt;%\n  mutate(Set = \"Test\")\n\n# Combine training and test predictions into a single data frame\npredictions_train_test_df &lt;- bind_rows(train_predictions, test_predictions) %&gt;%\n  rename(Observed = Y, Predicted = .pred)\n\n\n# Create scatter plot with different colors for training and test data\nggplot(predictions_train_test_df, aes(x = Observed, y = Predicted, color = Set, shape = Set)) +\n  geom_point(alpha = 0.7) + # Points for training and test predictions\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") + # 45-degree line\n  scale_x_continuous(limits = c(0, 6000)) + \n  scale_y_continuous(limits = c(0, 6000)) +\n  labs(title = \"Observed vs Predicted Values (Y ~ all predictors): Training vs Test\",\n       x = \"Observed Values\",\n       y = \"Predicted Values\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n# Compute RMSE for training data\nrmse_train &lt;- train_predictions %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  pull(.estimate)\n\n# Compute RMSE for test data\nrmse_test &lt;- test_predictions %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  pull(.estimate)\n\n# Print RMSE results\ncat(\"Model 6 (Y ~ all predictors) Evaluation:\\n\")\n\nModel 6 (Y ~ all predictors) Evaluation:\n\ncat(\"Training RMSE:\", round(rmse_train, 2), \"\\n\")\n\nTraining RMSE: 627.44 \n\ncat(\"Test RMSE:\", round(rmse_test, 2), \"\\n\")\n\nTest RMSE: 519.6 \n\n\nThe figure shows that the observed and predicted values for the test data are mixed in with the train data, which suggests that the model performs well."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#overall-model-assessment",
    "href": "fitting-exercise/fitting-exercise.html#overall-model-assessment",
    "title": "Fitting Exercise",
    "section": "Overall model assessment",
    "text": "Overall model assessment\nBoth models (Y ~ DOSE and Y ~ all predictors) are better than the null model as the RMSE of these models is lower than the null model.\nThe first model with only dose as a predictor can improve results over the null model, which suggests dose has an impact on the value of Y.\nThe second model with all predictors performs the best with the lowest RMSE. The predicted values for test data using the second model also indicate good performance. Interestingly, the RMSE for test data is even lower than the training data. The lower RMSE may be caused by random variation since the data set is small."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yufei Wu’s Data Analysis Portfolio",
    "section": "",
    "text": "Hello!\n\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nHave fun!"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-rayleen-lewis.",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-rayleen-lewis.",
    "title": "R Coding Exercise",
    "section": "This section contributed by Rayleen Lewis.",
    "text": "This section contributed by Rayleen Lewis.\n\nModule 3 Part 2: Exploration of movielens data\nI chose to explore the ‘movielens’ dataset. This contains information on movie titles, release year, genre, and ratings (including user who rated the movie and a time stamp for the rating).\nHere’s a summary of the data structure and variables:\n\n#data structure and summary\nstr(movielens)\n\n'data.frame':   100004 obs. of  7 variables:\n $ movieId  : int  31 1029 1061 1129 1172 1263 1287 1293 1339 1343 ...\n $ title    : chr  \"Dangerous Minds\" \"Dumbo\" \"Sleepers\" \"Escape from New York\" ...\n $ year     : int  1995 1941 1996 1981 1989 1978 1959 1982 1992 1991 ...\n $ genres   : Factor w/ 901 levels \"(no genres listed)\",..: 762 510 899 120 762 836 81 762 844 899 ...\n $ userId   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ rating   : num  2.5 3 3 2 4 2 2 2 3.5 2 ...\n $ timestamp: int  1260759144 1260759179 1260759182 1260759185 1260759205 1260759151 1260759187 1260759148 1260759125 1260759131 ...\n\nsummary(movielens)\n\n    movieId          title                year     \n Min.   :     1   Length:100004      Min.   :1902  \n 1st Qu.:  1028   Class :character   1st Qu.:1987  \n Median :  2406   Mode  :character   Median :1995  \n Mean   : 12549                      Mean   :1992  \n 3rd Qu.:  5418                      3rd Qu.:2001  \n Max.   :163949                      Max.   :2016  \n                                     NA's   :7     \n                  genres          userId        rating        timestamp        \n Drama               : 7757   Min.   :  1   Min.   :0.500   Min.   :7.897e+08  \n Comedy              : 6748   1st Qu.:182   1st Qu.:3.000   1st Qu.:9.658e+08  \n Comedy|Romance      : 3973   Median :367   Median :4.000   Median :1.110e+09  \n Drama|Romance       : 3462   Mean   :347   Mean   :3.544   Mean   :1.130e+09  \n Comedy|Drama        : 3272   3rd Qu.:520   3rd Qu.:4.000   3rd Qu.:1.296e+09  \n Comedy|Drama|Romance: 3204   Max.   :671   Max.   :5.000   Max.   :1.477e+09  \n (Other)             :71588                                                    \n\n\n\n\nData exploration\nI noticed in the summary above that genre appears to have many categories that overlap (e.g., a genre is listed as Comedy|Drama|Romance).\n\ngenre_options &lt;- movielens %&gt;%\n  group_by(genres) %&gt;%\n  summarize(n = n()) %&gt;%\n  arrange(desc(n))\ngenre_options\n\n# A tibble: 901 × 2\n   genres                               n\n   &lt;fct&gt;                            &lt;int&gt;\n 1 Drama                             7757\n 2 Comedy                            6748\n 3 Comedy|Romance                    3973\n 4 Drama|Romance                     3462\n 5 Comedy|Drama                      3272\n 6 Comedy|Drama|Romance              3204\n 7 Crime|Drama                       2367\n 8 Action|Adventure|Sci-Fi           2146\n 9 Action|Adventure|Sci-Fi|Thriller  1453\n10 Action|Crime|Thriller             1441\n# ℹ 891 more rows\n\n\nThere are approximately 900 different entries for genre, many of which include multiple genres. It is important to note that the genres always have a capitalized first letter and appear in alphabetical order (e.g., Crime|Drama|Film-Noir|Romance). I will need to get this variable into a usable format during the data cleaning process.\n\n\nData cleaning\nI’m interested in whether the mean movie rating has increased over time, especially as special effects have improved. Since this is particularly relevant for action movies, I’m interested in seeing if this association differs between action and romance movies (a genre where special effects don’t usually matter).\nAnother thing noticed during the data exploration was that this dataset spans from 1902 to 2016. The first use of computer animation in a movie didn’t occur until 1965, so I will also exclude movies prior to 1960.\nTo get the data into a usable format to answer these questions, I conducted the following steps:\n\nDropped unnecessary variables (title, userID, and timestamp)\nRemoved observations where the movie year as missing (i.e., NA)\nRemoved rows with a year prior to 1960\nCreate indicator variables for action movies and romance movies\nCreate a categorical year variable to represent decade the movie came out (for exploratory purposes)\n\n\n#Creating new, clean dataset classed movies from movielens\nmovies &lt;- movielens %&gt;% \n  #Removing unnecessary variables\n  select(movieId, year, genres, rating) %&gt;%\n  #Deleting rows missing year information\n  filter(!is.na(year)) %&gt;%\n  #Deleting rows with year &lt; 1960\n  filter(year &gt;= 1960) %&gt;%\n  #Creating action indicator variable, 1 if Action is in the genre list, 0 otherwise\n  mutate(action = if_else(grepl(\"Action\", genres), 1, 0)) %&gt;%\n  #Creating romance indicator variable, 1 if romance is in the genre list, 0 otherwise\n  mutate(romance = if_else(grepl(\"Romance\", genres), 1, 0)) %&gt;%\n  #Creating 4 level indicator that combines action and romance indicators\n  mutate(act_rom = if_else(action == 0 & romance ==0, 0, \n                           if_else(action == 1 & romance == 0, 1, \n                                   if_else(action == 0 & romance == 1, 2,\n                                           if_else(action == 1 & romance == 1, 3, NA))))) %&gt;%\n  #Creating categorical year variable grouped by decade\n  mutate(year_cat = case_when(year &lt; 1970 ~ 1,\n                              year &gt;= 1970 & year &lt; 1980 ~ 2,\n                              year &gt;= 1980 & year &lt; 1990 ~ 3,\n                              year &gt;= 1990 & year &lt; 2000 ~ 4,\n                              year &gt;= 2000 & year &lt; 2010 ~ 5,\n                              year &gt;= 2010 ~ 6))\n\n#Confirming structure and summary of the new dataset movies\nstr(movies)\n\n'data.frame':   94651 obs. of  8 variables:\n $ movieId : int  31 1061 1129 1172 1263 1293 1339 1343 1371 1405 ...\n $ year    : int  1995 1996 1981 1989 1978 1982 1992 1991 1979 1996 ...\n $ genres  : Factor w/ 901 levels \"(no genres listed)\",..: 762 899 120 762 836 762 844 899 485 359 ...\n $ rating  : num  2.5 3 2 4 2 2 3.5 2 2.5 1 ...\n $ action  : num  0 0 1 0 0 0 0 0 0 0 ...\n $ romance : num  0 0 0 0 0 0 1 0 0 0 ...\n $ act_rom : num  0 0 1 0 0 0 2 0 0 0 ...\n $ year_cat: num  4 4 3 3 2 3 4 4 2 4 ...\n\nsummary(movies)\n\n    movieId            year                       genres          rating     \n Min.   :     1   Min.   :1960   Drama               : 7187   Min.   :0.500  \n 1st Qu.:  1073   1st Qu.:1989   Comedy              : 6621   1st Qu.:3.000  \n Median :  2471   Median :1996   Comedy|Romance      : 3761   Median :4.000  \n Mean   : 13017   Mean   :1994   Drama|Romance       : 3240   Mean   :3.523  \n 3rd Qu.:  5507   3rd Qu.:2001   Comedy|Drama        : 3197   3rd Qu.:4.000  \n Max.   :163949   Max.   :2016   Comedy|Drama|Romance: 3039   Max.   :5.000  \n                                 (Other)             :67606                  \n     action          romance          act_rom          year_cat    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:3.000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :4.000  \n Mean   :0.2817   Mean   :0.1866   Mean   :0.6549   Mean   :3.984  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:5.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.0000   Max.   :6.000  \n                                                                   \n\n\nNext I want to confirm that the new indicator variables appeared to work by spot checking the new dataset.\n\n#Rows with Action included in the genre all have a value of 1 for action and 0 if they do not have action in the genre. Same goes for romance.\nmovies %&gt;%\n  select(genres, action, romance, act_rom) %&gt;%\n  head(20)\n\n                                        genres action romance act_rom\n1                                        Drama      0       0       0\n2                                     Thriller      0       0       0\n3             Action|Adventure|Sci-Fi|Thriller      1       0       1\n4                                        Drama      0       0       0\n5                                    Drama|War      0       0       0\n6                                        Drama      0       0       0\n7              Fantasy|Horror|Romance|Thriller      0       1       2\n8                                     Thriller      0       0       0\n9                             Adventure|Sci-Fi      0       0       0\n10            Adventure|Animation|Comedy|Crime      0       0       0\n11                       Action|Crime|Thriller      1       0       1\n12                     Action|Adventure|Sci-Fi      1       0       1\n13                            Adventure|Comedy      0       0       0\n14                    Action|Adventure|Fantasy      1       0       1\n15 Adventure|Animation|Children|Comedy|Fantasy      0       0       0\n16                Drama|Horror|Sci-Fi|Thriller      0       0       0\n17             Adventure|Comedy|Fantasy|Sci-Fi      0       0       0\n18                              Comedy|Western      0       0       0\n19                   Action|Adventure|Thriller      1       0       1\n20                               Drama|Romance      0       1       2\n\n#Movies with both action and romance in the genre have values of 1 for both indicator variables and 3 for combined variable\nmovies %&gt;%\n  select(genres, action, romance, act_rom) %&gt;%\n  filter(genres == \"Action|Adventure|Comedy|Romance|Thriller\") %&gt;%\n  head(10)\n\n                                     genres action romance act_rom\n1  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n2  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n3  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n4  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n5  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n6  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n7  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n8  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n9  Action|Adventure|Comedy|Romance|Thriller      1       1       3\n10 Action|Adventure|Comedy|Romance|Thriller      1       1       3\n\n\nThis looks good, movies with Action in the genre (even multi-genre movies) have a value of 1 for the action indicator. Same goes for romance movies for the romance indicator. Movies not in these genres have a 0 for the respective indicator variable.\n\n\nExploratory figures\nThese figures are being used to explore the whether movie ratings have increased with increasing use of computer graphics. The first figure is a stacked bar chart with year on the x-axis and the proportion of each rating on the y-axis. Ratings are color coded. The second is a boxplot of ratings by decade of release. These are plotted using the data overall and by genre (action/romance). As a note, since these are meant to be exploratory, I did not make them “pretty” by adjusting labels or aesthetic features.\n\n#Figure 1: Stacked bar chart looking at proportion of each rating by year\nmovies_sorted &lt;- movies %&gt;% arrange(rating)\nggplot(movies_sorted, aes(x=year, y = rating, fill=factor(rating))) + geom_bar(position=\"fill\", stat=\"identity\") +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(title = \"Proportion of each rating by year\", y=\"Proportion of each rating\")\n\n\n\n\n\n\n\n\nOverall, it seems like the average movie rating is likely decreasing since the cumulative proportion of ratings 4-5 has decreased with time.\n\n#Creating boxing plot of median movie rating by decade - overall\nggplot(movies, aes(x=factor(year_cat), y=rating)) + geom_boxplot() +\n  labs(title=\"Median rating by decade of release date (1=1960s, 6=2010s)\")\n\n\n\n\n\n\n\n\nThe median rating has also decreased somewhat over time. It also looks like ratings have become less varied (tighter IQR) with time.\nThe next set of figures are meant to be used to understand differences in the association between rating and year by genre (action vs romance).\n\n#Figure 2: Stacked bar chart looking at proportion of each rating by year for action movies and romance movies only\nmovies_sorted_act_rom &lt;- movies %&gt;% arrange(rating) %&gt;% filter(act_rom == 1 | act_rom ==2)\nggplot(movies_sorted_act_rom, aes(x=year, y = rating, fill=factor(rating))) + geom_bar(position=\"fill\", stat=\"identity\") +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(title = \"Proportion of each rating by year\", y=\"Proportion of each rating\")+\n  facet_grid(~act_rom)\n\n\n\n\n\n\n\n\nThere aren’t any particularly obvious patterns in ratings by movie type (action on the left, romance on the right).\n\n#Creating boxing plot of median movie rating by decade - by genre (action and romance)\nggplot(movies_sorted_act_rom, aes(x=factor(year_cat), y=rating)) + geom_boxplot() +\n  labs(title=\"Median rating by decade of release date (panel 1 = action, panel 2 = romance) (1=1960s, 6=2010s)\")+\n  facet_grid(~act_rom)\n\n\n\n\n\n\n\n\nSimilarly, the median (IQR) ratings are similar between the two genres.\n\n\nStatisical models\nTo test the first research question, I performed a simple linear regression of rating as the outcome and year as the predictor. As a note, rating is not continuous, it is ordinal. It wold be more appropriate to use a ordinal logistic regression, but I’m assuming this is outside the scope of this class and simple linear regression will be acceptable for demonstration purposes.\n\n#Simple linear regression model modeling rating by year\nmovies_model1 &lt;- lm(rating ~ year, movies)\n#Getting model results\nsummary(movies_model1)\n\n\nCall:\nlm(formula = rating ~ year, data = movies)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3342 -0.5356  0.2291  0.5820  1.6725 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 21.5698460  0.6399726    33.7   &lt;2e-16 ***\nyear        -0.0090488  0.0003209   -28.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.057 on 94649 degrees of freedom\nMultiple R-squared:  0.008332,  Adjusted R-squared:  0.008321 \nF-statistic: 795.2 on 1 and 94649 DF,  p-value: &lt; 2.2e-16\n\n\nYear was significantly negatively associated with mean move rating (P &lt; 0.001). This association was in the opposite direction than my hypothesis. Maybe as computer graphics were used more and more in movies, peoples’ expectations got higher and higher making it harder to get a high rating?\nTo look at the second question, I performed a multivariable linear regression with an interaction between year and my action/romance indicator. I also restricted the dataset to only movies who were either action or romance (movies that were both or neither were excluded).\n\n#Multivariable linear regression model modeling rating by year\nmovies_model2 &lt;- lm(rating ~ year + act_rom + act_rom:year, movies_sorted_act_rom)\n#Getting model results\nsummary(movies_model2)\n\n\nCall:\nlm(formula = rating ~ year + act_rom + act_rom:year, data = movies_sorted_act_rom)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2709 -0.5347  0.1652  0.6246  1.7666 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  33.396689   3.100212  10.772  &lt; 2e-16 ***\nyear         -0.015054   0.001553  -9.691  &lt; 2e-16 ***\nact_rom      -9.718532   2.171454  -4.476 7.64e-06 ***\nyear:act_rom  0.004913   0.001088   4.515 6.35e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.058 on 40184 degrees of freedom\nMultiple R-squared:  0.0088,    Adjusted R-squared:  0.008726 \nF-statistic: 118.9 on 3 and 40184 DF,  p-value: &lt; 2.2e-16\n\n\nThere was a significant difference in the effect of year on ratings for romance and action movies (P &lt; 0.001 - from interaction term). As expected, year had a stronger effect on action movies than romance movies (based on the estimates from the table above - effect of a 1 year increase on mean rating is -0.015 + 0.0049 = -0.0101 for action movies (coded as 1) and -0.015 + 2*0.0049 = -0.0052 for romance movies (coded as 2)).\n\n\nConclusions\nAverage movie ratings have decreased over time since computer graphics began being used in movies. This association is stornger for action movies than romance movies."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "Inspiration\nWhen evaluating the therapeutic efficacy of an anti-tumor drug using animal models, we typically perform immunohistochemical staining on tumor tissues. Beyond capturing images of the stained slides, we also quantify the results, such as calculating the percentage of positive cells relative to the total number of cells in an image. This analysis is often conducted using an ImageJ plugin. However, after completing this module, I wonder if R could be used for this task instead. I use a online ki67 staining image to perform the analysis (ki67 staining is to measure how much the tumor is proliferating).\n\n\nLoad package\n\nlibrary(EBImage)\n\n\n\nLoad the image\n\nimg &lt;- readImage(\"ki67.jpg\")\ndisplay(img)\n\n\n\n\n\n\n\n\nAs the figure shows, the positive cells are in brown color, darker than the negative cells (which are blue), so I will convert the image to grayscale and use different threshold to count the positive and total cells.\n\n\nThresholding to Identify Cells\nI will threshold the grayscale image to identify areas that likely correspond to cells.\n\n# Convert the image to grayscale\nimg_gray &lt;- channel(img, \"grey\")\n\n# Display the grayscale image\ndisplay(img_gray)\n\n\n\n\n\n\n\n# Thresholding to isolate cell regions (background vs. cells)\nthreshold_value &lt;- 0.6  # Adjust this value to make sure all the cells will be counted\nbinary_img &lt;- img_gray &lt; threshold_value\n\n# Display the binary image\ndisplay(binary_img)\n\n\n\n\n\n\n\n\n\n\nSegmenting the Cells\nI will use connected component analysis to identify and count individual cells, getting the total number of cells.\n\n# Perform connected component labeling to segment the cells\nlabel_img &lt;- bwlabel(binary_img)\n\n# Display the labeled image\ndisplay(label_img)\n\n\n\n\n\n\n\n# Count the total number of cells\ntotal_cells &lt;- max(label_img)  # The maximum label value represents the total number of cells\nprint(paste(\"Total number of cells: \", total_cells))\n\n[1] \"Total number of cells:  2693\"\n\n\n\n\nCounting Positive Cells\nNext, I want to identify the positive cells (brown stained) by applying a second threshold to the grayscale image.\n\n# Threshold the grayscale image to identify positive cells\nthreshold_brown_value &lt;- 0.3  # Adjust based on the intensity of the brown staining\npositive_cells_img &lt;- img_gray &lt; threshold_brown_value  # Darker pixels are considered positive\n\n# Display the adjusted image\ndisplay(positive_cells_img)\n\n\n\n\n\n\n\n# Perform connected component labeling for positive cells\npositive_label_img &lt;- bwlabel(positive_cells_img)\n\n# Count the number of positive cells\npositive_cells &lt;- max(positive_label_img)  # The maximum label value for positive cells\nprint(paste(\"Number of positive cells: \", positive_cells))\n\n[1] \"Number of positive cells:  626\"\n\n\n\n\nCalculating the Percentage of Positive Cells\n\n# Calculate the percentage of positive cells\npercentage_positive &lt;- (positive_cells / total_cells) * 100\nprint(paste(\"Percentage of positive cells: \", round(percentage_positive, 2), \"%\"))\n\n[1] \"Percentage of positive cells:  23.25 %\"\n\n\n\n\nResult\nThe percentage of positive cells is 23.25%, which is comparable to the result obtained from ImageJ (23.01%). This suggests that this approach could serve as an alternative to ImageJ, particularly when analyzing a large number of images, as it allows for an easy workflow by only manually changing the thresholds."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Me and my cat\n\n\n\nHi, I’m Yufei! I’m a second-year PhD student in Chemistry with a research focus on drug delivery systems for cancer treatment. While my background might be a bit unusual in this class, I’m also pursuing a secondary master’s degree in Statistics to broaden my skill set. I’m deeply fascinated by data analysis and passionate about conducting interdisciplinary research that bridges my expertise in chemistry and biology with statistical tools. I believe MADA presents an excellent opportunity to strengthen my coding skills and advance my data analysis capabilities."
  },
  {
    "objectID": "aboutme.html#brief-introduction",
    "href": "aboutme.html#brief-introduction",
    "title": "About me",
    "section": "",
    "text": "Me and my cat\n\n\n\nHi, I’m Yufei! I’m a second-year PhD student in Chemistry with a research focus on drug delivery systems for cancer treatment. While my background might be a bit unusual in this class, I’m also pursuing a secondary master’s degree in Statistics to broaden my skill set. I’m deeply fascinated by data analysis and passionate about conducting interdisciplinary research that bridges my expertise in chemistry and biology with statistical tools. I believe MADA presents an excellent opportunity to strengthen my coding skills and advance my data analysis capabilities."
  },
  {
    "objectID": "aboutme.html#skills",
    "href": "aboutme.html#skills",
    "title": "About me",
    "section": "Skills",
    "text": "Skills\nI’ve taken several courses in the Statistics department and am familiar with both R and Python. As part of a course project, I developed an interactive R Shiny app to visualize cancer incidence across various cancer sites, differentiated by age groups, sex and states. I’m excited to learn more about programming and statistical modeling through this course."
  },
  {
    "objectID": "aboutme.html#fun-facts",
    "href": "aboutme.html#fun-facts",
    "title": "About me",
    "section": "Fun facts",
    "text": "Fun facts\nI have a lovely cat named Mercury. She’s very playful and always excited to see people, especially when my friends visit. Whenever I come home from school, she immediately jumps onto the windowsill to watch me park."
  },
  {
    "objectID": "aboutme.html#interesting-website",
    "href": "aboutme.html#interesting-website",
    "title": "About me",
    "section": "Interesting website",
    "text": "Interesting website\nI’m excited to share a fantastic YouTube channel with you called StatQuest. It offers clear and engaging explanations of statistics and data science concepts in a fun and approachable way. The host, Josh Starmer, breaks down complex theories and methodologies into simple and easy-to-understand pieces, making challenging topics accessible. I’m especially impressed with the quirky animations and the host’s enthusiastic style, which turn dry, technical content into an enjoyable learning experience."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "Description\nThe data I use for this exercise is called “An aggregated dataset of serially collected influenza A virus morbidity and titer measurements from virus-infected ferrets.” I got it from National Center for Immunization and Respiratory Diseases. It contains data of 728 ferrets inoculated with 126 unique IAV, including morbidity, mortality, and viral titer data. Here is the link to this dataset: https://data.cdc.gov/National-Center-for-Immunization-and-Respiratory-D/An-aggregated-dataset-of-serially-collected-influe/cr56-k9wj/about_data\n\n\nLoad and process the data\n\n# Load package\nlibrary(here)\nlibrary(dplyr)\ninstall.packages(\"rlang\")\n\nThe following package(s) will be installed:\n- rlang [1.1.5]\nThese packages will be installed into \"~/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing rlang ...                          OK [linked from cache]\nSuccessfully installed 1 package in 2.6 milliseconds.\n\ninstall.packages(\"rlang\", dependencies = TRUE)\n\nThe following package(s) will be installed:\n- rlang [1.1.5]\nThese packages will be installed into \"~/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing rlang ...                          OK [linked from cache]\nSuccessfully installed 1 package in 2.6 milliseconds.\n\ninstall.packages(\"gt\", dependencies = TRUE)\n\nThe following package(s) will be installed:\n- gt [0.11.1]\nThese packages will be installed into \"~/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing gt ...                             OK [linked from cache]\nSuccessfully installed 1 package in 3.2 milliseconds.\n\nlibrary(gt)\nlibrary(ggplot2)\ninstall.packages(\"gridExtra\")\n\nThe following package(s) will be installed:\n- gridExtra [2.3]\nThese packages will be installed into \"~/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing gridExtra ...                      OK [linked from cache]\nSuccessfully installed 1 package in 3.1 milliseconds.\n\nlibrary(gridExtra)\n\ninstall.packages(\"ggpubr\")\n\nThe following package(s) will be installed:\n- ggpubr [0.6.0]\nThese packages will be installed into \"~/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggpubr ...                         OK [linked from cache]\nSuccessfully installed 1 package in 3.6 milliseconds.\n\nlibrary(ggpubr)\nlibrary(tidyverse)\ninstall.packages(\"cli\")\n\nThe following package(s) will be installed:\n- cli [3.6.4]\nThese packages will be installed into \"~/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing cli ...                            OK [linked from cache]\nSuccessfully installed 1 package in 3.6 milliseconds.\n\nlibrary(cli)\n\n# Load data\niavdata &lt;- read.csv(here(\"cdcdata-exercise\", \"influenza_data.csv\"))\n\n# Get an overview of the data\nstr(iavdata)       # Structure of the dataset\n\n'data.frame':   728 obs. of  29 variables:\n $ Ferret     : chr  \"F1\" \"F2\" \"F3\" \"F4\" ...\n $ Virus      : chr  \"A/Turkey/VA/4529/2002\" \"A/Turkey/VA/4529/2002\" \"A/Turkey/VA/4529/2002\" \"A/Turkey/VA/4529/2002\" ...\n $ inoc_dose  : num  7 7 7 7 7 7 7 7 7 7 ...\n $ units      : chr  \"EID\" \"EID\" \"EID\" \"EID\" ...\n $ expt       : chr  \"path\" \"path\" \"path\" \"DC\" ...\n $ lethal     : chr  \"false\" \"false\" \"false\" \"false\" ...\n $ lethal_day : int  0 0 0 0 0 0 0 0 0 0 ...\n $ NW_typical : chr  \"true\" \"true\" \"true\" \"true\" ...\n $ RD_trans   : chr  NA NA NA NA ...\n $ HPAI       : chr  \"false\" \"false\" \"false\" \"false\" ...\n $ HPAI_MBAA  : chr  \"false\" \"false\" \"false\" \"false\" ...\n $ HA         : chr  \"H7\" \"H7\" \"H7\" \"H7\" ...\n $ NA.        : chr  \"N2\" \"N2\" \"N2\" \"N2\" ...\n $ Origin     : chr  \"avian\" \"avian\" \"avian\" \"avian\" ...\n $ wt_loss    : num  6.7 11.8 15.4 5.9 7.9 6.1 3.5 22.5 15 14.9 ...\n $ wt_loss_day: int  7 4 6 11 5 9 11 7 7 8 ...\n $ temp       : num  0.5 0.9 2 1 1 0.5 0.5 1.1 1.1 1 ...\n $ temp_day   : int  4 7 8 9 3 3 7 7 7 1 ...\n $ temp_5     : num  0.5 0.7 1.8 0.3 1 0.5 0.3 0.1 0.6 1 ...\n $ temp_5_day : int  4 2 2 3 3 3 1 3 3 1 ...\n $ d1_inoc    : num  6.5 5.5 6.25 6.75 6.25 7.25 5.75 5.75 4.75 6.5 ...\n $ d2_inoc    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ d3_inoc    : num  4.5 6.5 5.5 6.75 6.5 6.75 5.75 5.5 3.75 5.25 ...\n $ d4_inoc    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ d5_inoc    : num  4.75 4.75 5.75 5.75 4.75 4.5 5.75 5.5 5.25 6.5 ...\n $ d6_inoc    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ d7_inoc    : num  1.75 1.5 1.5 3.25 1.5 1.5 1.5 2.25 1.5 3.25 ...\n $ d8_inoc    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ d9_inoc    : num  1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 ...\n\nsummary(iavdata)   # Summary statistics\n\n    Ferret             Virus             inoc_dose        units          \n Length:728         Length:728         Min.   :5.000   Length:728        \n Class :character   Class :character   1st Qu.:6.000   Class :character  \n Mode  :character   Mode  :character   Median :6.000   Mode  :character  \n                                       Mean   :6.089                     \n                                       3rd Qu.:6.000                     \n                                       Max.   :7.000                     \n                                                                         \n     expt              lethal            lethal_day      NW_typical       \n Length:728         Length:728         Min.   : 0.000   Length:728        \n Class :character   Class :character   1st Qu.: 0.000   Class :character  \n Mode  :character   Mode  :character   Median : 0.000   Mode  :character  \n                                       Mean   : 1.049                     \n                                       3rd Qu.: 0.000                     \n                                       Max.   :13.000                     \n                                                                          \n   RD_trans             HPAI            HPAI_MBAA              HA           \n Length:728         Length:728         Length:728         Length:728        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     NA.               Origin             wt_loss        wt_loss_day    \n Length:728         Length:728         Min.   : 0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 4.000  \n Mode  :character   Mode  :character   Median : 9.300   Median : 7.000  \n                                       Mean   : 9.914   Mean   : 6.431  \n                                       3rd Qu.:14.425   3rd Qu.: 9.000  \n                                       Max.   :27.500   Max.   :14.000  \n                                                                        \n      temp          temp_day          temp_5        temp_5_day      d1_inoc    \n Min.   :0.000   Min.   : 0.000   Min.   :0.000   Min.   :0.00   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:1.000   1st Qu.:1.00   1st Qu.:4.85  \n Median :1.500   Median : 2.000   Median :1.500   Median :2.00   Median :5.78  \n Mean   :1.586   Mean   : 3.082   Mean   :1.501   Mean   :2.04   Mean   :5.81  \n 3rd Qu.:2.200   3rd Qu.: 4.000   3rd Qu.:2.025   3rd Qu.:3.00   3rd Qu.:6.75  \n Max.   :4.000   Max.   :14.000   Max.   :4.000   Max.   :5.00   Max.   :9.25  \n                                                                 NA's   :115   \n    d2_inoc         d3_inoc         d4_inoc         d5_inoc     \n Min.   :3.500   Min.   :1.980   Min.   :1.980   Min.   :1.500  \n 1st Qu.:5.500   1st Qu.:4.500   1st Qu.:4.250   1st Qu.:4.470  \n Median :5.813   Median :5.340   Median :5.250   Median :5.100  \n Mean   :5.931   Mean   :5.296   Mean   :5.008   Mean   :5.048  \n 3rd Qu.:6.500   3rd Qu.:6.250   3rd Qu.:5.750   3rd Qu.:5.750  \n Max.   :8.750   Max.   :8.750   Max.   :7.500   Max.   :9.500  \n NA's   :607     NA's   :115     NA's   :613     NA's   :121    \n    d6_inoc         d7_inoc         d8_inoc         d9_inoc     \n Min.   :1.301   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.250   1st Qu.:1.000   1st Qu.:1.500   1st Qu.:1.000  \n Median :4.500   Median :1.500   Median :1.500   Median :1.500  \n Mean   :4.123   Mean   :1.997   Mean   :1.543   Mean   :1.327  \n 3rd Qu.:5.250   3rd Qu.:2.500   3rd Qu.:1.500   3rd Qu.:1.500  \n Max.   :6.750   Max.   :7.000   Max.   :3.500   Max.   :4.750  \n NA's   :621     NA's   :150     NA's   :632     NA's   :180    \n\ndim(iavdata)       # Dimensions (rows and columns)\n\n[1] 728  29\n\n# Check missing values\ncolSums(is.na(iavdata))\n\n     Ferret       Virus   inoc_dose       units        expt      lethal \n          0           0           0           0           0           0 \n lethal_day  NW_typical    RD_trans        HPAI   HPAI_MBAA          HA \n          0           0         498           0           0           0 \n        NA.      Origin     wt_loss wt_loss_day        temp    temp_day \n          0           0           0           0           0           0 \n     temp_5  temp_5_day     d1_inoc     d2_inoc     d3_inoc     d4_inoc \n          0           0         115         607         115         613 \n    d5_inoc     d6_inoc     d7_inoc     d8_inoc     d9_inoc \n        121         621         150         632         180 \n\n# Pick a few vairables\niavdata2 &lt;- iavdata %&gt;% select(lethal, Origin, wt_loss, wt_loss_day, temp)\n\n# View the structure of the new data frame\nstr(iavdata2)\n\n'data.frame':   728 obs. of  5 variables:\n $ lethal     : chr  \"false\" \"false\" \"false\" \"false\" ...\n $ Origin     : chr  \"avian\" \"avian\" \"avian\" \"avian\" ...\n $ wt_loss    : num  6.7 11.8 15.4 5.9 7.9 6.1 3.5 22.5 15 14.9 ...\n $ wt_loss_day: int  7 4 6 11 5 9 11 7 7 8 ...\n $ temp       : num  0.5 0.9 2 1 1 0.5 0.5 1.1 1.1 1 ...\n\n# Display the first few rows\nhead(iavdata2)\n\n  lethal Origin wt_loss wt_loss_day temp\n1  false  avian     6.7           7  0.5\n2  false  avian    11.8           4  0.9\n3  false  avian    15.4           6  2.0\n4  false  avian     5.9          11  1.0\n5  false  avian     7.9           5  1.0\n6  false  avian     6.1           9  0.5\n\n\nThe variables d1_inoc, d2_inoc, …, d9_inoc have missing values, likely due to the every-other-day sampling schedule or the euthanasia of ferrets that reached humane endpoints in the study. However, I will not focus on these variables, as I find other variables more interesting for exploration. I choose the variables lethal, Origin, wt_loss, wt_loss_day, and temp to do exploration.\nlethal: if the ferret survived the 14 day p.i. inoculation period (FALSE) or was humanely. euthanized between days 1-14 p.i. due to reaching experimental endpoints (TRUE).\nOrigin: Categorical column indicating the host origin of the inoculating virus.\nwt_loss: Numerical column that specifies the maximum percentage weight loss.\nwt_loss_day :Numerical column that indicates the day p.i. the maximum percentage weight loss reported in wt_loss was detected.\ntemp: Numerical column that specifies the maximum increase in degrees Celsius.\n\n\nExplore the data\nExplore the categorical variables\n\n# Make table summarizing the Origin lethal rate\n# Encode lethal to be logical\niavdata2 &lt;- iavdata2 %&gt;%\n  mutate(lethal = as.logical(lethal)) \n\n# Create a summary table with lethal rate (where lethal is TRUE)\nexploratory_table &lt;- iavdata2 %&gt;%\n  group_by(Origin) %&gt;%\n  summarise(\n    Total = n(),\n    Lethal_True = sum(lethal, na.rm = TRUE),  # Ensure TRUE values are counted\n    Lethal_Rate = round((Lethal_True / Total) * 100, 2)  # Calculate percentage\n  ) %&gt;%\n  arrange(desc(Lethal_Rate))  # Sort by Lethal Rate\n\n# Display the table\nexploratory_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Lethal (TRUE) Rate by Origin\"\n  ) %&gt;%\n  cols_label(\n    Origin = \"Origin\",\n    Total = \"Total Cases\",\n    Lethal_True = \"Lethal (TRUE) Count\",\n    Lethal_Rate = \"Lethal Rate (%)\"\n  )\n\n\n\n\n\n\n\nLethal (TRUE) Rate by Origin\n\n\nOrigin\nTotal Cases\nLethal (TRUE) Count\nLethal Rate (%)\n\n\n\n\navian\n384\n97\n25.26\n\n\nhuman\n180\n7\n3.89\n\n\nvariant\n146\n4\n2.74\n\n\ncanine\n3\n0\n0.00\n\n\nswine\n15\n0\n0.00\n\n\n\n\n\n\n# Make table summarizing the composition of the Origin\norigin_summary &lt;- iavdata2 %&gt;%\n  group_by(Origin) %&gt;%\n  summarise(\n    Count = n(),  # Total observations per origin\n    Percentage = round((Count / nrow(iavdata2)) * 100, 2)  # Calculate percentage\n  ) %&gt;%\n  arrange(desc(Count))  # Sort by highest count\n\n# Display the table\norigin_summary %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Composition of Origin\"\n  ) %&gt;%\n  cols_label(\n    Origin = \"Origin\",\n    Count = \"Count\",\n    Percentage = \"Percentage (%)\"\n  )\n\n\n\n\n\n\n\nComposition of Origin\n\n\nOrigin\nCount\nPercentage (%)\n\n\n\n\navian\n384\n52.75\n\n\nhuman\n180\n24.73\n\n\nvariant\n146\n20.05\n\n\nswine\n15\n2.06\n\n\ncanine\n3\n0.41\n\n\n\n\n\n\n\nExplore continuous variables\n\n# Plot the distribution and summarize mean and standard deviation of wt_loss, wt_loss_day, and temp\n# Define a function to plot and summarize a variable\nplot_and_summarize &lt;- function(data, variable) {\n  # Create histogram with density curve\n  p &lt;- ggplot(data, aes(x = !!sym(variable))) +\n    geom_histogram(aes(y = ..density..), bins = 30, fill = \"pink\", color = \"black\") +\n    geom_density(color = \"skyblue\", size = 1) +\n    labs(title = paste(\"Distribution of\", variable), x = variable, y = \"Density\") +\n    theme_minimal()\n  \n  # Print the plot\n  print(p)\n  \n  # Calculate mean and standard deviation\n  summary_stats &lt;- data %&gt;%\n    summarise(\n      Mean = mean(!!sym(variable), na.rm = TRUE),\n      SD = sd(!!sym(variable), na.rm = TRUE)\n    )\n  \n  return(summary_stats)\n}\n\n# Plot and summarize wt_loss\nwt_loss_summary &lt;- plot_and_summarize(iavdata2, \"wt_loss\")\n\n\n\n\n\n\n\nprint(wt_loss_summary)\n\n      Mean       SD\n1 9.914286 6.668187\n\n# Plot and summarize wt_loss_day\nwt_loss_day_summary &lt;- plot_and_summarize(iavdata2, \"wt_loss_day\")\n\n\n\n\n\n\n\nprint(wt_loss_day_summary)\n\n      Mean       SD\n1 6.431319 3.193995\n\n# Plot and summarize temp\ntemp_summary &lt;- plot_and_summarize(iavdata2, \"temp\")\n\n\n\n\n\n\n\nprint(temp_summary)\n\n      Mean        SD\n1 1.585989 0.7760666\n\n# Box plot of lethal vs. wt_loss\nggplot(iavdata2, aes(x = as.factor(lethal), y = wt_loss)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Box Plot of Lethal vs. Maximum Percentage Weight Loss\",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Weight Loss (wt_loss)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Box plot of lethal vs. wt_loss_day\nggplot(iavdata2, aes(x = as.factor(lethal), y = wt_loss_day)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Box Plot of Lethal vs. Day of Maximum Percentage Weight Loss\",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Weight Loss Day (wt_loss_day)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Box plot of lethal vs. temp\nggplot(iavdata2, aes(x = as.factor(lethal), y = temp)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Box Plot of Lethal vs. Maximum Increase in Degrees Celsius\",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Temperature (temp)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThis section contributed by Muhammad Nasir\nIn this part, I would like to create a new dataset that similiar with the existing dataset. Let assume that the existing dataset is not possible to share publicly. Therefore, we want to create sythetic data to share wihch is similiar with the original dataset.\n\nSummarise the data\nBefore stsrting to sythesize a new dataset from the existing dataset, I would like to check the summary of the data and explore the data more.\n\nsummary(iavdata2)\n\n   lethal           Origin             wt_loss        wt_loss_day    \n Mode :logical   Length:728         Min.   : 0.000   Min.   : 0.000  \n FALSE:620       Class :character   1st Qu.: 4.800   1st Qu.: 4.000  \n TRUE :108       Mode  :character   Median : 9.300   Median : 7.000  \n                                    Mean   : 9.914   Mean   : 6.431  \n                                    3rd Qu.:14.425   3rd Qu.: 9.000  \n                                    Max.   :27.500   Max.   :14.000  \n      temp      \n Min.   :0.000  \n 1st Qu.:1.000  \n Median :1.500  \n Mean   :1.586  \n 3rd Qu.:2.200  \n Max.   :4.000  \n\n\nI want to see more information\n\norigin_summary %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Composition of Origin\"\n  ) %&gt;%\n  cols_label(\n    Origin = \"Origin\",\n    Count = \"Count\",\n    Percentage = \"Percentage (%)\"\n  )\n\n\n\n\n\n\n\nComposition of Origin\n\n\nOrigin\nCount\nPercentage (%)\n\n\n\n\navian\n384\n52.75\n\n\nhuman\n180\n24.73\n\n\nvariant\n146\n20.05\n\n\nswine\n15\n2.06\n\n\ncanine\n3\n0.41\n\n\n\n\n\n\n\nI want to create table Lethal vs origin\n\ntable(iavdata2$lethal, iavdata2$Origin)\n\n       \n        avian canine human swine variant\n  FALSE   287      3   173    15     142\n  TRUE     97      0     7     0       4\n\n\nI want to find the SD of continouse variables\n\nnumeric_cols &lt;- sapply(iavdata2, is.numeric) # select numeric column only \nstd_dev &lt;- apply(iavdata2[, numeric_cols], 2, sd)\n\nprint(std_dev)\n\n    wt_loss wt_loss_day        temp \n  6.6681872   3.1939950   0.7760666 \n\n\nI want to create dataset based on the condition of existing dataset. I get help from AI to generate the dataset with the following prompt.\nI have a dataset called iavdata2. It contains 728 observations and 5 variabels, the followings are the condition of my dataset: 1. lethal (Mode: Logical with 620 Falses and 108 True) 2. origin (with Lenght 728, class: Character, and mode: Character) 3. wt_loss (min. 0.000, 1st Qu.: 4.800, median : 9.300, mean : 9.914, 3rd Qu. : 14.425, and Max. : 27.500, sd: 6.6681872) 4. wt_loss_day with (min. 0.000, 1st Qu. 4.000, Median: 7.000), Mean: 6.431, 3rd Qu. 9.000, max. 14.000, sd: 3.1939950) 5. temp (min. : 0.000, 1st Qu. : 1.000, Median: 1.500, Mean : 1.586, 3rd Qu. : 2.200, max.: 4.000, sd: 0.7760666) 6. Composition of origin: Avian 384 (52.75%), human 180 (24.73%), variant 146 (20.05%), swine 15 (2.06%), canine 3 (0.41%) 7. lethal condition: False ( avian 287, canine 3, human 173, swine 15, variant 142), and true ( avian 97, canine 0, human 7, swine 0, variant 4)\nPlease help me create a new sythetic dataset which is similiar with my existing dataset, based on information provided. Generate code in R\n\n# Set the number of observations for the synthetic dataset\nn &lt;- 728\n\n# 1. Generate the 'lethal' variable (Logical: 620 Falses and 108 Trues)\nlethal &lt;- sample(c(FALSE, TRUE), size = n, replace = TRUE, prob = c(620/728, 108/728))\n\n# 2. Generate the 'origin' variable (Character with given proportions)\norigin &lt;- sample(c(\"Avian\", \"Human\", \"Variant\", \"Swine\", \"Canine\"), \n                 size = n, replace = TRUE, \n                 prob = c(384/728, 180/728, 146/728, 15/728, 3/728))\n\n# 3. Generate the 'wt_loss' variable (Continuous variable with mean 9.914 and sd 6.668)\nwt_loss &lt;- rnorm(n, mean = 9.914, sd = 6.668)\n\n# 4. Generate the 'wt_loss_day' variable (Continuous variable with mean 6.431 and sd 3.194)\nwt_loss_day &lt;- rnorm(n, mean = 6.431, sd = 3.194)\n\n# 5. Generate the 'temp' variable (Continuous variable with mean 1.586 and sd 0.776)\ntemp &lt;- rnorm(n, mean = 1.586, sd = 0.776)\n\n# Combine all variables into a data frame\nsyn_iavdata &lt;- data.frame(\n  lethal = lethal,\n  origin = origin,\n  wt_loss = wt_loss,\n  wt_loss_day = wt_loss_day,\n  temp = temp\n)\n\n\nhead(syn_iavdata)\n\n  lethal  origin   wt_loss wt_loss_day      temp\n1  FALSE Variant  9.116984    5.527572 1.3326351\n2  FALSE Variant 16.584922    9.240962 1.6178540\n3  FALSE Variant 10.120183    6.878953 1.8732801\n4   TRUE   Avian 12.757522   10.714237 0.9224663\n5  FALSE Variant  6.215084   10.296665 1.1012872\n6  FALSE   Human 12.308585    7.855492 0.8810121\n\n\nCheck dimension of the generated dataset\n\ndim(syn_iavdata)\n\n[1] 728   5\n\n\n\n# Display a summary of the synthetic dataset\nsummary(syn_iavdata)\n\n   lethal           origin             wt_loss         wt_loss_day    \n Mode :logical   Length:728         Min.   :-12.130   Min.   :-2.807  \n FALSE:616       Class :character   1st Qu.:  5.399   1st Qu.: 4.503  \n TRUE :112       Mode  :character   Median :  9.550   Median : 6.449  \n                                    Mean   :  9.616   Mean   : 6.501  \n                                    3rd Qu.: 14.052   3rd Qu.: 8.642  \n                                    Max.   : 31.162   Max.   :16.636  \n      temp        \n Min.   :-0.8543  \n 1st Qu.: 1.0065  \n Median : 1.5658  \n Mean   : 1.5680  \n 3rd Qu.: 2.1247  \n Max.   : 3.9088  \n\n\n\nData exploration and compare between original dan sythetic data\n\n\n\nExplore the data\nExplore the categorical variables\n\n# Make table summarizing the Origin lethal rate\n# Encode lethal to be logical\nsyn_iavdata &lt;- syn_iavdata %&gt;%\n  mutate(lethal = as.logical(lethal)) \n\n# Create a summary table with lethal rate (where lethal is TRUE)\nexploratory_table &lt;- syn_iavdata %&gt;%\n  group_by(origin) %&gt;%\n  summarise(\n    Total = n(),\n    Lethal_True = sum(lethal, na.rm = TRUE),  # Ensure TRUE values are counted\n    Lethal_Rate = round((Lethal_True / Total) * 100, 2)  # Calculate percentage\n  ) %&gt;%\n  arrange(desc(Lethal_Rate))  # Sort by Lethal Rate\n\n# Display the table# syn_iavdataDisplay the table\nexploratory_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Lethal (TRUE) Rate by Origin\"\n  ) %&gt;%\n  cols_label(\n    origin = \"Origin\",\n    Total = \"Total Cases\",\n    Lethal_True = \"Lethal (TRUE) Count\",\n    Lethal_Rate = \"Lethal Rate (%)\"\n  )\n\n\n\n\n\n\n\nLethal (TRUE) Rate by Origin\n\n\nOrigin\nTotal Cases\nLethal (TRUE) Count\nLethal Rate (%)\n\n\n\n\nCanine\n5\n1\n20.00\n\n\nAvian\n359\n63\n17.55\n\n\nHuman\n185\n28\n15.14\n\n\nVariant\n167\n19\n11.38\n\n\nSwine\n12\n1\n8.33\n\n\n\n\n\n\n# Make table summarizing the composition of the Origin\norigin_summary &lt;- syn_iavdata %&gt;%\n  group_by(origin) %&gt;%\n  summarise(\n    Count = n(),  # Total observations per origin\n    Percentage = round((Count / nrow(iavdata2)) * 100, 2)  # Calculate percentage\n  ) %&gt;%\n  arrange(desc(Count))  # Sort by highest count\n\n# Display the table\norigin_summary %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Composition of Origin\"\n  ) %&gt;%\n  cols_label(\n    origin = \"Origin\",\n    Count = \"Count\",\n    Percentage = \"Percentage (%)\"\n  )\n\n\n\n\n\n\n\nComposition of Origin\n\n\nOrigin\nCount\nPercentage (%)\n\n\n\n\nAvian\n359\n49.31\n\n\nHuman\n185\n25.41\n\n\nVariant\n167\n22.94\n\n\nSwine\n12\n1.65\n\n\nCanine\n5\n0.69\n\n\n\n\n\n\n\nExplore the numeric variables\nI want to explore both dataset by comparing both datasets\n\n# Box plot of lethal vs. wt_loss_day\nplot1 &lt;- ggplot(iavdata2, aes(x = as.factor(lethal), y = wt_loss_day)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Lethal vs. Day of Weight Loss (original data)\",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Weight Loss Day (wt_loss_day)\"\n  ) +\n  theme_minimal()\n\nplot2 &lt;- ggplot(syn_iavdata, aes(x = as.factor(lethal), y = wt_loss_day)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Lethal vs. Day of Weight Loss (Sythetic Data) \",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Weight Loss Day (wt_loss_day)\"\n  ) +\n  theme_minimal()\n\n\n\nlethal_day &lt;- grid.arrange(plot1, plot2, ncol = 2) # to put those plots side by side \n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\",\"pictures\",\"lethal_day.png\")\nggsave(filename = figure_file, plot=lethal_day) \n\nSaving 7 x 5 in image\n\n\n\n# Box plot of lethal vs. temp\nplot3 &lt;- ggplot(iavdata2, aes(x = as.factor(lethal), y = temp)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Lethal vs. Temperature  (Original Data)\",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Temperature (temp)\"\n  ) +\n  theme_minimal()\n\n\nplot4 &lt;- ggplot(syn_iavdata, aes(x = as.factor(lethal), y = temp)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\", outlier.size = 2) +\n  labs(\n    title = \"Lethal vs. Temperature (sythetic Data)\",\n    x = \"Lethal (0 = False, 1 = True)\",\n    y = \"Temperature (temp)\"\n  ) +\n  theme_minimal()\n\nlethal_temp &lt;- grid.arrange(plot3, plot4, ncol = 2)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\",\"pictures\",\"lethal_temp.png\")\nggsave(filename = figure_file, plot=lethal_temp) \n\nSaving 7 x 5 in image\n\n\nWeight loss: original VS sythetics\n\nhis_wtloss_ori &lt;- ggplot(iavdata2, aes(x = wt_loss)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"green\", color = \"black\", alpha = 0.7) +  # Density-based histogram\n  geom_density(color = \"blue\", size = 1) +  # Overlay density curve\n  labs(title = \"Distribution of wt_loss (original)\", x = \"wt_loss\", y = \"Density\") + \n  theme_minimal()\n\nhis_wtloss_syn &lt;- ggplot(syn_iavdata, aes(x = wt_loss)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"pink\", color = \"black\", alpha = 0.7) +  # Density-based histogram\n  geom_density(color = \"blue\", size = 1) +  # Overlay density curve\n  labs(title = \"Distribution of wt_loss (synthetic)\", x = \"wt_loss\", y = \"Density\") + \n  theme_minimal()\n\nhis_wtloss &lt;- grid.arrange(his_wtloss_ori, his_wtloss_syn, ncol = 2)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\",\"pictures\",\"his_wtlossp.png\")\nggsave(filename = figure_file, plot=his_wtloss)\n\nSaving 7 x 5 in image\n\n\nWight loss day: Original data vs sythetic data\n\nhis_day_ori &lt;- ggplot(iavdata2, aes(x = wt_loss)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"purple\", color = \"black\", alpha = 0.7) +  # Density-based histogram\n  geom_density(color = \"blue\", size = 1) +  # Overlay density curve\n  labs(title = \"Distribution of Day Weight loss (original)\", x = \"wt_day_loss\", y = \"Density\") + \n  theme_minimal()\n\nhis_day_syn &lt;- ggplot(syn_iavdata, aes(x = wt_loss)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +  # Density-based histogram\n  geom_density(color = \"blue\", size = 1) +  # Overlay density curve\n  labs(title = \"Distribution of Day Weight loss  (synthetic)\", x = \"wt_day_loss\", y = \"Density\") + \n  theme_minimal()\n\nhis_wt_day_loss &lt;- grid.arrange(his_day_ori, his_day_syn, ncol = 2)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\",\"pictures\",\"his_wt_day_loss.png\")\nggsave(filename = figure_file, plot=his_wt_day_loss)\n\nSaving 7 x 5 in image\n\n\nTemperature: original VS sythetics\n\nhis_temp_ori &lt;- ggplot(iavdata2, aes(x = wt_loss)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"red\", color = \"black\", alpha = 0.7) +  # Density-based histogram\n  geom_density(color = \"blue\", size = 1) +  # Overlay density curve\n  labs(title = \"Distribution of temperature (original)\", x = \"temperature\", y = \"Density\") + \n  theme_minimal()\n\nhis_temp_syn &lt;- ggplot(syn_iavdata, aes(x = wt_loss)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"yellow\", color = \"black\", alpha = 0.7) +  # Density-based histogram\n  geom_density(color = \"blue\", size = 1) +  # Overlay density curve\n  labs(title = \"Distribution of temperature (synthetic)\", x = \"temperature\", y = \"Density\") + \n  theme_minimal()\n\nhis_temp &lt;- grid.arrange(his_temp_ori, his_temp_syn, ncol = 2)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\",\"pictures\",\"his_temp .png\")\nggsave(filename = figure_file, plot=his_temp )\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Create a graph\nI choose to reproduce a graph in the ariticle “Be Suspicious Of Online Movie Ratings, Especially Fandango’s” from FiveThirtyEight. The graph is shown below:\n\n\n\nI first sent the graph to ChatGPT and asked it to re-create the original graph. Here’s the initial code I received to make the graph.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n# Load dataset\nscore &lt;- read.csv(\"fandango_score_comparison.csv\")\n\n# Create a dataframe with normalized scores\nscore_normalized &lt;- score %&gt;%\n  select(Fandango_Stars, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round) %&gt;%\n  gather(key = \"Source\", value = \"Rating\")  # Reshape data to long format\n\n# Create the plot\nggplot(score_normalized, aes(x = Rating, fill = Source)) +\n  geom_density(alpha = 0.6) + \n  scale_fill_manual(values = c(\"Fandango_Ratingvalue\" = \"red\", \n                               \"IMDB_norm_round\" = \"yellow\", \n                               \"Metacritic_norm_round\" = \"green\", \n                               \"RT_norm_round\" = \"blue\", \n                               \"RT_user_norm_round\" = \"lightblue\", \n                               \"Metacritic_user_norm_round\" = \"lightgreen\")) +\n  theme_minimal() +\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of films in theaters in 2015\",\n       x = \"Normalized Rating\", \n       y = \"Density\") +\n  theme(legend.position = \"top\") +\n  scale_x_continuous(breaks = seq(0, 5, by = 0.5)) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThe graph is not similar to the original one, so I used some prompts to let ChatGPT revise the code: Use percentage as y axis; Discrete points that are connected by straight lines instead of smooth curves; Use use geom_ribbon to create shadows under each line. After iterations, I got the code as below:\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load dataset\nscore &lt;- read.csv(\"fandango_score_comparison.csv\")\n\n# Create a dataframe with normalized scores\nscore_normalized &lt;- score %&gt;%\n  select(Fandango_Stars, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round) %&gt;%\n  gather(key = \"Source\", value = \"Rating\")  # Reshape data to long format\n\n# Create a count of ratings for each source\nrating_counts &lt;- score_normalized %&gt;%\n  group_by(Source, Rating) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%  # Avoid the grouped output warning\n  ungroup() %&gt;%\n  group_by(Source) %&gt;%\n  mutate(percentage = count / sum(count) * 100)  # Convert to percentage\n\n# Extend the data to include the full range of x-axis\nrating_counts_extended &lt;- rating_counts %&gt;%\n  complete(Rating = seq(0.5, 5, by = 0.5), fill = list(count = 0)) %&gt;%\n  group_by(Source) %&gt;%\n  mutate(percentage = count / sum(count) * 100)  # Recalculate percentage after filling\n\n# Define color palette for the shaded areas (to match the original graph)\ncolors &lt;- c(\"Fandango_Stars\" = \"red\", \n            \"IMDB_norm_round\" = \"yellow\", \n            \"Metacritic_norm_round\" = \"green\", \n            \"RT_norm_round\" = \"blue\", \n            \"RT_user_norm_round\" = \"lightblue\", \n            \"Metacritic_user_norm_round\" = \"lightgreen\")\n\n# Create the plot with shaded areas beneath the lines using geom_ribbon\nggplot(rating_counts_extended, aes(x = Rating, y = percentage, group = Source, fill = Source)) +\n  geom_ribbon(aes(ymin = 0, ymax = percentage), alpha = 0.4) +  # Add ribbon for shadow effect\n  geom_line(size = 1) +  # Sharp lines\n  scale_x_continuous(breaks = seq(0, 5, by = 0.5)) +  # X-axis breaks at 0.5 intervals\n  scale_fill_manual(values = colors) +  # Use the custom color palette for the fill\n  scale_color_manual(values = colors) +  # Use the same color for lines\n  theme_minimal() +\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of films in theaters in 2015\",\n       x = \"Normalized Rating\", \n       y = \"Percentage\") +\n  theme(legend.position = \"top\") +\n  theme(legend.title = element_blank()) +  # Clean up legend\n  expand_limits(x = c(0.5, 5))  # Extend the x-axis range\n\n\n\n\n\n\n\n\nNow it looks almost good, and then I will make modifications manually to make it more similar to the original graph.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggthemes)\n\n# Load dataset\nscore &lt;- read.csv(\"fandango_score_comparison.csv\")\n\n# Create a dataframe with normalized scores\nscore_normalized &lt;- score %&gt;%\n  select(Fandango_Stars, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round) %&gt;%\n  gather(key = \"Source\", value = \"Rating\")  # Reshape data to long format\n\n# Create a count of ratings for each source\nrating_counts &lt;- score_normalized %&gt;%\n  group_by(Source, Rating) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%  # Avoid the grouped output warning\n  ungroup() %&gt;%\n  group_by(Source) %&gt;%\n  mutate(percentage = count / sum(count) * 100)  # Convert to percentage\n\n# Extend the data to include the full range of x-axis\nrating_counts_extended &lt;- rating_counts %&gt;%\n  complete(Rating = seq(0.5, 5, by = 0.5), fill = list(count = 0)) %&gt;%\n  group_by(Source) %&gt;%\n  mutate(percentage = count / sum(count) * 100)  # Recalculate percentage after filling\n\n# Define color palette for both the ribbons and the lines to match\ncolors_line &lt;- c(\"Fandango_Stars\" = \"#fb4d2f\", \n            \"IMDB_norm_round\" = \"#f4c026\", \n            \"Metacritic_norm_round\" = \"#a04f9c\", \n            \"RT_norm_round\" = \"#6a6a6a\", \n            \"RT_user_norm_round\" = \"#37a4da\", \n            \"Metacritic_user_norm_round\" = \"#aeca92\")\n\ncolors_fill &lt;- c(\"Fandango_Stars\" = \"#fb4d2f\", \n            \"IMDB_norm_round\" = \"#6a6a6a\", \n            \"Metacritic_norm_round\" = \"#6a6a6a\", \n            \"RT_norm_round\" = \"#6a6a6a\", \n            \"RT_user_norm_round\" = \"#6a6a6a\", \n            \"Metacritic_user_norm_round\" = \"#6a6a6a\")\n\n# Create a custom label map\nlabel_map &lt;- c(\"Fandango_Stars\" = \"Fandango\", \n               \"RT_norm_round\" = \"Rotten Tomatoes\", \n               \"RT_user_norm_round\" = \"Rotten Tomatoes users\", \n               \"Metacritic_norm_round\" = \"Metacritic\", \n               \"Metacritic_user_norm_round\" = \"Metacritic users\", \n               \"IMDB_norm_round\" = \"IMDb users\")\n\n# Add hjust and vjust columns to control text label positioning\nrating_counts_extended &lt;- rating_counts_extended %&gt;%\n  mutate(\n    hjust = case_when(\n      Source == \"Fandango_Stars\" ~ -0.3, \n      Source == \"IMDB_norm_round\" ~ 1.2,\n      Source == \"Metacritic_norm_round\" ~ 4,\n      Source == \"RT_norm_round\" ~ 5.5,\n      Source == \"RT_user_norm_round\" ~ 2.2,\n      Source == \"Metacritic_user_norm_round\" ~ 1.2\n    ),\n    vjust = case_when(\n      Source == \"Fandango_Stars\" ~ 1.5,\n      Source == \"IMDB_norm_round\" ~ 1.5,\n      Source == \"Metacritic_norm_round\" ~ 2,\n      Source == \"RT_norm_round\" ~ 1.5,\n      Source == \"RT_user_norm_round\" ~ 1.8,\n      Source == \"Metacritic_user_norm_round\" ~ 1.5\n    )\n  )\n\n# Create the plot with shaded areas beneath the lines using geom_ribbon\nggplot(rating_counts_extended, aes(x = Rating, y = percentage, group = Source, fill = Source, color = Source)) +\n  geom_ribbon(aes(ymin = 0, ymax = percentage), alpha = 0.3) +  # Add ribbon for shadow effect\n  geom_line(size = 0.5) +  # Sharp lines for each source\n  scale_x_continuous(breaks = seq(0, 5)) +  # X-axis breaks at 0.5 intervals\n  scale_fill_manual(values = colors_fill) +  # Use the custom color palette for the fill\n  scale_color_manual(values = colors_line) +  # Ensure the lines share the same colors as the ribbons\n  theme_minimal() +\n  theme_fivethirtyeight()+\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of 146 films in theaters in 2015 that\nhad 30+ reviews on Fandango.com\",\n       x=\"\",\n       y = \"Percentage\") +\n  theme(legend.position = \"none\",  # Hide the legend\n    plot.title = element_text(size = 18, face = \"bold\", color = \"black\", hjust = 0), \n    # Title settings\n    plot.subtitle = element_text(size = 14, color = \"black\", hjust = 0),  \n    # Subtitle settings\n    plot.margin = margin(20, 20, 20, 20)  # Adjust plot margin\n    ) +  # Hide the legend\n  expand_limits(x = c(0, 5)) +  # Extend the x-axis range\n  # Add custom labels near the peak of each line with correct colors\n  geom_text(data = rating_counts_extended %&gt;%\n              group_by(Source) %&gt;%\n              slice(which.max(percentage)),  # Take the highest point for each line\n            aes(label = label_map[Source], color = Source, hjust = hjust, vjust = vjust),\n            size = 3)\n\n\n\n\n\n\n\n\nHere’ the original graph for comparison:\n\n\n\nOverall, the reproduced graph is similar to the original one, despite some imperfections, like the x-axis being numbers instead of the star images in the original graph.\n\n\nCreate a table\nI use the prompts below to let ChatGPT generate table for me: Make table showing summary statistics such as mean, median, min, max of the Fandango_Stars, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round. * Bold the column headers for Mean (SD), Median (Q1, Q3), Min, and Max. * Highlight the “Mean (SD)” cell specifically for the Fandango platform. * Additional text outside of the regular table structure like spanning column headers or correctly attributed footnotes * A descriptive caption explaining the useful information summary that the table represents. Since I tried for a few hours and ChatGPT still could not add a column of sparklines for me, I made plots of distribution separately and saved them as pictures and then inserted them into the column.\n\n# Load libraries\n#library(kableExtra)\nlibrary(gt)\nlibrary(dplyr)\n\n# Read the data\ndata &lt;- read.csv(\"fandango_score_comparison.csv\")\n\n# Descriptive platform names\nplatform_names &lt;- c(\n  \"Fandango_Stars\" = \"Fandango\",\n  \"RT_norm_round\" = \"Rotten Tomatoes\",\n  \"RT_user_norm_round\" = \"Rotten Tomatoes users\",\n  \"Metacritic_norm_round\" = \"Metacritic\",\n  \"Metacritic_user_norm_round\" = \"Metacritic users\",\n  \"IMDB_norm_round\" = \"IMDb users\"\n)\n\n# List of variables to plot\nvariables &lt;- c(\"Fandango_Stars\", \"RT_norm_round\", \"RT_user_norm_round\", \n               \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\")\n\n# Function to generate and save distribution plots\ngenerate_distribution_plots &lt;- function(variable) {\n  # Create the plot (density plot in this case)\n  p &lt;- ggplot(data, aes_string(x = variable)) + \n    geom_density(fill = \"skyblue\", alpha = 0.5) + \n    theme_minimal()\n  \n  # Save the plot as PNG\n  file_name &lt;- paste0( variable, \"_distribution.png\")\n  ggsave(file_name, plot = p, width = 8, height = 6)\n  \n  # Return the file path\n  return(file_name)\n}\n\n# Generate and save the distribution plots for each variable\nplot_paths &lt;- sapply(variables, generate_distribution_plots)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n# Function to generate image file paths (replace with real image paths)\ngenerate_image_path &lt;- function(platform_name) {\n  # For demonstration, I use a placeholder image name (replace with your actual images)\n  image_urls &lt;- c(\n    \"Fandango\" = \"Fandango_Stars_distribution.png\",\n    \"Rotten Tomatoes\" = \"RT_norm_round_distribution.png\",\n    \"Rotten Tomatoes users\" = \"RT_user_norm_round_distribution.png\",\n    \"Metacritic\" = \"IMDB_norm_round_distribution.png\",\n    \"Metacritic users\" = \"Metacritic_norm_round_distribution.png\",\n    \"IMDb users\" = \"Metacritic_user_norm_round_distribution.png\"\n  )\n  \n  return(image_urls[platform_name])\n}\n\n# Compute summary statistics for each platform\nsummary_stats &lt;- data %&gt;%\n  summarise(\n    `Platform` = platform_names[\"Fandango_Stars\"],\n    `Mean (SD)` = paste0(round(mean(Fandango_Stars), 2), \" (\", round(sd(Fandango_Stars), 2), \")\"),\n    `Median (Q1, Q3)` = paste0(round(median(Fandango_Stars), 2), \" (\", \n                               round(quantile(Fandango_Stars, 0.25), 2), \", \", \n                               round(quantile(Fandango_Stars, 0.75), 2), \")\"),\n    Min = round(min(Fandango_Stars), 2),\n    Max = round(max(Fandango_Stars), 2),\n    Distribution = generate_image_path(\"Fandango\")  # Add image path for Fandango\n  )\n\n# Platforms for which summary statistics are needed\nplatforms &lt;- c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \n               \"Metacritic_user_norm_round\", \"IMDB_norm_round\")\n\n# Compute summary statistics for each platform and append them\nfor (platform in platforms) {\n  stats &lt;- data %&gt;%\n    summarise(\n      `Platform` = platform_names[platform],\n      `Mean (SD)` = paste0(round(mean(get(platform)), 2), \" (\", round(sd(get(platform)), 2), \")\"),\n      `Median (Q1, Q3)` = paste0(round(median(get(platform)), 2), \" (\", \n                                 round(quantile(get(platform), 0.25), 2), \", \", \n                                 round(quantile(get(platform), 0.75), 2), \")\"),\n      Min = round(min(get(platform)), 2),\n      Max = round(max(get(platform)), 2),\n      Distribution = generate_image_path(platform_names[platform])  # Add image path for each platform\n    )\n  \n  # Append the stats to summary_stats\n  summary_stats &lt;- bind_rows(summary_stats, stats)\n}\n\n# Create the table with summary statistics and image column\nfinal_table &lt;- summary_stats %&gt;%\n  gt() %&gt;%\n  # Adding header that spans multiple columns\n  tab_header(\n    title = \"Summary Statistics for Movie Rating Platforms\",\n    subtitle = \"Comparison of Ratings on Different Platforms\"\n  ) %&gt;%\n  tab_spanner(\n    label = md(\"**Summary Statistics**\"),\n    columns = c(`Mean (SD)`, `Median (Q1, Q3)`, Min, Max)\n  ) %&gt;%\n  # Adding a descriptive footnote for statistics\n  tab_footnote(\n    footnote = md(\"Summary statistics (mean, median, min, max) for different movie rating platforms.\"),\n    locations = cells_column_labels(columns = `Mean (SD)`)\n  ) %&gt;%\n  # Adding a source note with explanation\n  tab_source_note(\n    source_note = md(\"**Table 1 caption:** This table compares summary statistics like mean, median, min, and max for ratings across various platforms.\")\n  ) %&gt;%\n  # Bold specific column labels: Mean (SD), Median (Q1, Q3), Min, Max\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(columns = c(\"Mean (SD)\", \"Median (Q1, Q3)\", \"Min\", \"Max\"))\n  ) %&gt;%\n  # Highlighting the \"Mean (SD)\" cell for \"Fandango\"\n  tab_style(\n    style = cell_fill(color = \"lightblue\"),  # Highlight color\n    locations = cells_body(\n      rows = summary_stats$Platform == \"Fandango\",  # Targeting Fandango row\n      columns = \"Mean (SD)\"\n    )\n  ) %&gt;%\n  # Displaying images in the new 'Image' column\n  fmt_image(\n    columns = \"Distribution\",\n    rows = everything(),\n    path = NULL,  # Path is not required if you're using URLs\n    file_pattern = \"{x}\",\n    encode = TRUE\n  )\n\n# Display the table\nfinal_table\n\n\n\n\n  \n    \n      Summary Statistics for Movie Rating Platforms\n    \n    \n      Comparison of Ratings on Different Platforms\n    \n    \n      Platform\n      \n        Summary Statistics\n      \n      Distribution\n    \n    \n      Mean (SD)1\n      Median (Q1, Q3)\n      Min\n      Max\n    \n  \n  \n    Fandango\n4.09 (0.54)\n4 (3.5, 4.5)\n3.0\n5.0\n\n    Rotten Tomatoes\n3.07 (1.51)\n3 (1.5, 4.5)\n0.5\n5.0\n\n    Rotten Tomatoes users\n3.23 (1.01)\n3.5 (2.5, 4)\n1.0\n4.5\n\n    Metacritic\n2.97 (0.99)\n3 (2.12, 4)\n0.5\n4.5\n\n    Metacritic users\n3.27 (0.79)\n3.5 (3, 4)\n1.0\n5.0\n\n    IMDb users\n3.38 (0.5)\n3.5 (3, 3.5)\n2.0\n4.5\n\n  \n  \n    \n      Table 1 caption: This table compares summary statistics like mean, median, min, and max for ratings across various platforms.\n    \n  \n  \n    \n      1 Summary statistics (mean, median, min, max) for different movie rating platforms."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. # I added two columns “Age” and “Employment Status” to the data sheet. “Age” includes numeric values and “Employment Status” includes “Employed”, “Unemployed”, and “Retired”."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/yufeiwu/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/yufeiwu/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/yufeiwu/Desktop/STAT master/MADA/yufeiwu-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name`   `Variable Definition`                   `Allowed Values`    \n  &lt;chr&gt;             &lt;chr&gt;                                   &lt;chr&gt;               \n1 Height            height in centimeters                   numeric value &gt;0 or…\n2 Weight            weight in kilograms                     numeric value &gt;0 or…\n3 Gender            identified gender (male/female/other)   M/F/O/NA            \n4 Age               age in years old                        numeric value &gt;0 or…\n5 Employment Status E (employed),U (unemployed), R(retired) E/U/R/NA            \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height              &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"…\n$ Weight              &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45,…\n$ Gender              &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\",…\n$ Age                 &lt;dbl&gt; 15, 22, 33, 45, 65, 72, 50, 36, 25, 17, 28, 54, 41…\n$ `Employment Status` &lt;chr&gt; \"U\", \"U\", \"E\", \"E\", \"R\", \"R\", \"E\", \"E\", \"U\", \"U\", …\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :15.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:25.75  \n Mode  :character   Median :  70.0   Mode  :character   Median :38.50  \n                    Mean   : 602.7                      Mean   :39.00  \n                    3rd Qu.:  90.0                      3rd Qu.:48.75  \n                    Max.   :7000.0                      Max.   :72.00  \n                    NA's   :1                                          \n Employment Status \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age `Employment Status`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;              \n1 180        80 M         15 U                  \n2 175        70 O         22 U                  \n3 sixty      60 F         33 E                  \n4 178        76 F         45 E                  \n5 192        90 NA        65 R                  \n6 6          55 F         72 R                  \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEmployment Status\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90.00\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n39.00\n17.27\n15\n25.75\n38.5\n48.75\n72\n▇▆▆▃▃\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEmployment Status\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n39.46\n17.89\n15\n25.00\n41\n50\n72\n▇▃▆▃▃\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEmployment Status\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n39.46\n17.89\n15\n25.00\n41\n50\n72\n▇▃▆▃▃\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEmployment Status\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n42.55\n17.60\n15\n30.5\n43\n52\n72\n▇▂▇▅▅\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEmployment Status\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n42.55\n17.60\n15\n30.5\n43\n52\n72\n▇▂▇▅▅\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEmployment Status\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n42.00\n16.90\n15\n36\n43\n50\n72\n▅▂▇▅▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Yufei Wu's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]